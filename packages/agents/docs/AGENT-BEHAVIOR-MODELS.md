# Guia de Termos e Sufixos em Modelos de IA

Este guia explica os principais termos, sufixos e conven√ß√µes encontrados em nomes de modelos de IA, especialmente √∫teis para agentes aut√¥nomos, workflows multimodais e automa√ß√£o Slice/ALIVE.

cd /caminho/para/user_data/models
# Exemplo para Hugging Face
wget https://huggingface.co/namespace/model1/resolve/main/model1.safetensors
wget https://huggingface.co/namespace/model2/resolve/main/model2.safetensors

---

## **1. instruct**
- **Significado:** Modelo ajustado (fine-tuned) para seguir instru√ß√µes humanas de forma clara, √∫til e segura.
- **Uso:** Ideal para agentes, automa√ß√£o, chatbots, workflows, tarefas orientadas a comandos.
- **Exemplo:** `Llama-3.1-8B-Instruct`, `Qwen2.5-Omni-7B-Instruct`
- **Dica:** Prompts diretos funcionam melhor: "Resuma o texto abaixo", "Traduza para ingl√™s", etc.

---

## **2. base**
- **Significado:** Modelo "cru", sem ajuste espec√≠fico para instru√ß√µes. Gera texto de forma livre, sem foco em seguir comandos.
- **Uso:** Pesquisa, gera√ß√£o criativa, experimenta√ß√£o, fine-tuning customizado.
- **Exemplo:** `Llama-2-7B-Base`, `bert-base-portuguese-cased`
- **Dica:** Pode ser menos confi√°vel para tarefas que exigem obedi√™ncia a comandos.

---

## **3. chat**
- **Significado:** Modelo ajustado para conversa√ß√£o, di√°logo e intera√ß√£o natural com humanos.
- **Uso:** Chatbots, assistentes virtuais, interfaces conversacionais.
- **Exemplo:** `Llama-2-7B-Chat`, `OpenChat-3.5`
- **Dica:** Responde de forma mais natural, mant√©m contexto de di√°logo.

---

## **4. vision**
- **Significado:** Modelo multimodal capaz de processar, analisar e entender imagens (al√©m de texto).
- **Uso:** An√°lise de imagens, OCR, perguntas sobre fotos, automa√ß√£o visual.
- **Exemplo:** `Llama-3-Vision`, `Qwen-Vision`, `Llama-Guard-3-11B-Vision`
- **Dica:** Permite prompts que combinam texto e imagem.

---

## **5. code**
- **Significado:** Modelo ajustado para tarefas de programa√ß√£o: gera√ß√£o, explica√ß√£o, refatora√ß√£o de c√≥digo.
- **Uso:** Assistentes de c√≥digo, automa√ß√£o de refatora√ß√£o, explica√ß√£o de trechos.
- **Exemplo:** `CodeLlama-34B`, `StarCoder`, `DeepSeek-Coder`
- **Dica:** Nem todo modelo code faz code completion puro; alguns s√£o melhores para explica√ß√£o e an√°lise.

---

## **6. scout**
- **Significado:** Termo menos padronizado, mas geralmente indica modelo/agente especializado em busca, explora√ß√£o, coleta de informa√ß√µes ou varredura de dados.
- **Uso:** Agentes de pesquisa, coleta de dados, crawling, an√°lise explorat√≥ria.
- **Exemplo:** `ScoutLM`, `DataScout` (nomes hipot√©ticos ou experimentais)
- **Dica:** Verifique sempre a documenta√ß√£o do modelo para entender o foco real.

---

## **7. omni**
- **Significado:** Indica modelo "universal" ou multimodal, capaz de lidar com m√∫ltiplos tipos de entrada (texto, imagem, √°udio, etc.).
- **Uso:** Agentes generalistas, automa√ß√£o multimodal.
- **Exemplo:** `Qwen2.5-Omni-7B`

---

## **8. paraphrase**
- **Significado:** Modelo treinado para reescrever frases mantendo o significado (par√°frase).
- **Uso:** Deduplica√ß√£o, reescrita, simplifica√ß√£o de texto.
- **Exemplo:** `paraphrase-multilingual-MiniLM-L12-v2`

---

## **9. multilingual / multi**
- **Significado:** Modelo treinado para m√∫ltiplos idiomas.
- **Uso:** Agentes globais, tradu√ß√£o, an√°lise de texto em diferentes l√≠nguas.
- **Exemplo:** `intfloat/multilingual-e5-large-instruct`

---

## **10. large / xl / xxl / 7B / 70B / 104B**
- **Significado:** Indica o tamanho do modelo (quantidade de par√¢metros). Quanto maior, mais "inteligente" e caro de rodar.
- **Uso:** Escolha conforme recursos dispon√≠veis e necessidade de performance.
- **Exemplo:** `Llama-2-70B`, `command-r-plus-104B`, `MiniLM-L12-v2`

---

## **11. mini / small / tiny**
- **Significado:** Modelos compactos, r√°pidos, ideais para edge, prototipa√ß√£o ou ambientes com poucos recursos.
- **Exemplo:** `MiniLM`, `TinyLlama`

---

## **12. instruct-v1, v2, etc.**
- **Significado:** Vers√£o do ajuste instruct. Vers√µes mais altas geralmente trazem melhorias de alinhamento e performance.
- **Exemplo:** `OpenHermes-2.5-Mistral-instruct-v2`

---

## **13. stt / tts**
- **Significado:**
  - **stt:** Speech-to-Text (transcri√ß√£o de √°udio para texto)
  - **tts:** Text-to-Speech (s√≠ntese de voz)
- **Exemplo:** `whisper_stt`, `silero_tts`

---

## **14. ner**
- **Significado:** Named Entity Recognition (Reconhecimento de Entidades Nomeadas)
- **Uso:** Extra√ß√£o de nomes, datas, locais, etc. de textos.
- **Exemplo:** `bertimbau-base-lener_br`

---

## **15. base64 / quantized / int4 / int8**
- **Significado:** Indica formato de compress√£o ou quantiza√ß√£o do modelo para rodar em hardware limitado.
- **Exemplo:** `Llama-2-7B-int4`

---

## **16. instruct, chat, base, vision, code, scout ‚Äî Resumo R√°pido**
| Sufixo   | Foco Principal                | Ideal para...                |
|----------|------------------------------|------------------------------|
| instruct | Seguir comandos/instru√ß√µes   | Agentes, automa√ß√£o, assistente|
| chat     | Conversa√ß√£o natural          | Chatbots, di√°logos           |
| base     | Gera√ß√£o livre, pesquisa      | Pesquisa, fine-tuning        |
| vision   | Imagem + texto (multimodal)  | OCR, an√°lise visual          |
| code     | Programa√ß√£o                  | Gera√ß√£o/explica√ß√£o de c√≥digo |
| scout    | Busca/explora√ß√£o             | Pesquisa, crawling           |

---

## **17. embedding**
- **Significado:** Representa√ß√£o vetorial densa de um texto, imagem ou outro dado, criada para capturar o significado sem√¢ntico e permitir compara√ß√£o eficiente.
- **Uso:** Busca sem√¢ntica, mem√≥ria vetorial, RAG (Retrieval Augmented Generation), deduplica√ß√£o, clustering, recomenda√ß√£o.
- **Exemplo de modelos:** `intfloat/multilingual-e5-large-instruct`, `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`, `jinaai/jina-embeddings-v3`
- **Como funciona:** O modelo converte textos (ou imagens) em vetores num√©ricos de alta dimens√£o. Vetores pr√≥ximos indicam conte√∫dos semanticamente similares.
- **Para agentes:** Essencial para mem√≥ria contextual, busca inteligente, sumariza√ß√£o baseada em contexto e integra√ß√£o de m√∫ltiplas fontes de informa√ß√£o.

---

## **18. guard**
- **Significado:** Modelos ou mecanismos projetados para filtrar, moderar, validar ou proteger a entrada/sa√≠da de outros modelos de IA, prevenindo respostas inadequadas, inseguras ou fora de pol√≠tica.
- **Uso:** Modera√ß√£o de conte√∫do, seguran√ßa, compliance, valida√ß√£o de prompts e respostas, prote√ß√£o contra alucina√ß√µes ou vazamento de dados sens√≠veis.
- **Exemplo de modelos:** `Llama-Guard-3-11B-Vision`, `Llama-Guard-7B`, sistemas de "prompt guard" ou "output guard".
- **Como funciona:** O guard atua como um "porteiro" ‚Äî recebe o texto (ou imagem) antes ou depois do modelo principal, avalia riscos, bloqueia ou ajusta conforme regras de seguran√ßa e pol√≠ticas definidas.
- **Para agentes:** Essencial em ambientes empresariais, aplica√ß√µes sens√≠veis, automa√ß√£o com exposi√ß√£o p√∫blica ou integra√ß√£o com usu√°rios finais.

---

## **19. token classification**
- **Significado:** Tarefa de IA/NLP onde cada token (palavra, subpalavra ou caractere) de um texto √© classificado individualmente, geralmente para identificar entidades, categorias ou fun√ß√µes gramaticais.
- **Uso:** Reconhecimento de Entidades Nomeadas (NER), an√°lise gramatical, extra√ß√£o de informa√ß√µes, segmenta√ß√£o de texto.
- **Exemplo de modelos:** `bert-base-cased` (usado para NER), `Luciano/bertimbau-base-lener_br`, modelos BERT, RoBERTa, etc. com cabe√ßote de classifica√ß√£o de token.
- **Como funciona:** O modelo processa o texto e atribui um r√≥tulo a cada token, como "B-PESSOA", "I-LOCAL", "O" (fora de entidade), etc.
- **Para agentes:** Fundamental para extra√ß√£o estruturada de dados, automa√ß√£o de compliance, an√°lise de documentos e integra√ß√£o com sistemas de informa√ß√£o.

---

## **20. Zero-Shot Image Classification**
- **Significado:** Classifica√ß√£o de imagens em categorias nunca vistas durante o treinamento, usando apenas descri√ß√µes textuais das classes.
- **Uso:** Classifica√ß√£o flex√≠vel, adapta√ß√£o a novos dom√≠nios sem re-treinar o modelo.
- **Exemplo de modelos:** `CLIP`, `BLIP`, `OpenCLIP`, `Florence`
- **Como funciona:** O modelo compara a imagem com descri√ß√µes textuais das classes e escolhe a mais similar.
- **Para agentes:** Permite classificar imagens em tempo real para categorias din√¢micas, √∫til em automa√ß√£o e an√°lise visual.

---

## **21. Fill-Mask**
- **Significado:** Tarefa de prever a(s) palavra(s) que preenchem uma lacuna (m√°scara) em uma frase.
- **Uso:** Autocompletar, corre√ß√£o, an√°lise de contexto, gera√ß√£o de texto.
- **Exemplo de modelos:** `bert-base-cased`, `roberta-base`, `distilbert-base-uncased`
- **Como funciona:** O modelo recebe um texto com um token especial (ex: [MASK]) e sugere as melhores op√ß√µes para preencher.
- **Para agentes:** √ötil para an√°lise de contexto, sugest√µes inteligentes e automa√ß√£o de preenchimento de dados.

---

## **22. Text Ranking**
- **Significado:** Ordena√ß√£o de textos (documentos, respostas, senten√ßas) por relev√¢ncia em rela√ß√£o a uma consulta.
- **Uso:** Busca sem√¢ntica, RAG, sistemas de recomenda√ß√£o, FAQ bots.
- **Exemplo de modelos:** `colbert`, `ANCE`, `sentence-transformers/ms-marco-TinyBERT-L-2-v2`
- **Como funciona:** O modelo avalia a similaridade entre a consulta e cada texto, retornando um ranking.
- **Para agentes:** Essencial para busca inteligente, prioriza√ß√£o de respostas e integra√ß√£o com bases de conhecimento.

---

## **23. sentence-similarity**
- **Significado:** Medida de qu√£o semanticamente pr√≥ximas s√£o duas senten√ßas/textos.
- **Uso:** Deduplica√ß√£o, busca, recomenda√ß√£o, clustering, an√°lise de pl√°gio.
- **Exemplo de modelos:** `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`, `intfloat/multilingual-e5-large-instruct`
- **Como funciona:** O modelo gera embeddings para cada senten√ßa e calcula a similaridade (ex: cosseno) entre os vetores.
- **Para agentes:** Permite comparar respostas, identificar redund√¢ncias e sugerir conte√∫dos relacionados.

---

## **24. Automatic Speech Recognition (ASR)**
- **Significado:** Convers√£o autom√°tica de fala (√°udio) em texto.
- **Uso:** Transcri√ß√£o, legendas, automa√ß√£o de atendimento, acessibilidade.
- **Exemplo de modelos:** `whisper`, `wav2vec2`, `silero_stt`
- **Para agentes:** Permite entrada de voz, automa√ß√£o de transcri√ß√µes e integra√ß√£o com sistemas de voz.

---

## **25. Text-to-Speech (TTS)**
- **Significado:** Convers√£o de texto em fala (√°udio).
- **Uso:** Leitura autom√°tica, acessibilidade, assistentes virtuais, rob√¥s.
- **Exemplo de modelos:** `silero_tts`, `coqui-ai/TTS`, `google-tts`
- **Para agentes:** Permite sa√≠da de voz, automa√ß√£o de respostas faladas e integra√ß√£o com sistemas de √°udio.

---

## **26. Text-to-Audio**
- **Significado:** Gera√ß√£o de √°udio a partir de texto, podendo incluir m√∫sica, efeitos sonoros ou fala.
- **Uso:** Cria√ß√£o de conte√∫do, automa√ß√£o multim√≠dia, gera√ß√£o de sons personalizados.
- **Exemplo de modelos:** `bark`, `audioldm`, `musicgen`
- **Para agentes:** Expande as capacidades multimodais, permitindo gera√ß√£o de sons al√©m da fala.

---

## **27. Audio Classification**
- **Significado:** Classifica√ß√£o de √°udios em categorias (ex: m√∫sica, fala, ru√≠do, emo√ß√£o, comando).
- **Uso:** Detec√ß√£o de eventos, automa√ß√£o de monitoramento, an√°lise de sentimentos em √°udio.
- **Exemplo de modelos:** `yamnet`, `panns`, `audioset`
- **Como funciona:** O modelo processa o √°udio e atribui um r√≥tulo ou probabilidade para cada classe.
- **Para agentes:** Permite automa√ß√£o de triagem, monitoramento ambiental e an√°lise de intera√ß√µes por √°udio.

---

## **28. Tabular Classification**
- **Significado:** Classifica√ß√£o de registros (linhas) em tabelas de dados estruturados (ex: CSV, SQL) em categorias pr√©-definidas.
- **Uso:** Diagn√≥stico m√©dico, detec√ß√£o de fraude, classifica√ß√£o de clientes, an√°lise de cr√©dito.
- **Exemplo de modelos:** `RandomForestClassifier`, `XGBoost`, `CatBoost`, `mljs/DecisionTreeClassifier`, modelos scikit-learn, AutoML.
- **Como funciona:** O modelo aprende padr√µes em colunas (features) para prever a classe de cada linha.
- **Para agentes:** Permite automa√ß√£o de decis√µes baseadas em dados tabulares, integra√ß√£o com sistemas empresariais e an√°lise de grandes volumes de dados estruturados.

---

## **29. Tabular Regression**
- **Significado:** Predi√ß√£o de valores num√©ricos cont√≠nuos a partir de dados tabulares estruturados.
- **Uso:** Previs√£o de pre√ßos, estimativa de demanda, an√°lise de risco, previs√£o de vendas.
- **Exemplo de modelos:** `RandomForestRegressor`, `XGBoostRegressor`, `mljs/DecisionTreeRegression`, modelos scikit-learn, AutoML.
- **Como funciona:** O modelo aprende rela√ß√µes entre features e o valor alvo, prevendo n√∫meros em vez de classes.
- **Para agentes:** Essencial para automa√ß√£o de previs√µes, an√°lise financeira, planejamento e otimiza√ß√£o de recursos.

---

## **30. Time Series Forecasting**
- **Significado:** Previs√£o de valores futuros em s√©ries temporais (dados ordenados no tempo), considerando padr√µes, tend√™ncias e sazonalidade.
- **Uso:** Previs√£o de demanda, estoque, clima, vendas, consumo de energia.
- **Exemplo de modelos:** `ARIMA`, `Prophet`, `LSTM`, `mljs/TimeSeries`, `scikit-learn`, `gluonts`, `DeepAR`.
- **Como funciona:** O modelo analisa o hist√≥rico temporal para prever pr√≥ximos valores, podendo incorporar m√∫ltiplas vari√°veis e ciclos.
- **Para agentes:** Fundamental para automa√ß√£o de planejamento, detec√ß√£o de anomalias, ajuste din√¢mico de recursos e tomada de decis√£o baseada em tempo.

---

## **31. OCR (Reconhecimento √ìptico de Caracteres)**
- **Categoria:** Computer Vision > Image-to-Text
- **Contexto:** OCR converte imagens (fotos, documentos escaneados, PDFs) em texto digital. √â fundamental para digitaliza√ß√£o de documentos, automa√ß√£o de entrada de dados e acessibilidade. Em frameworks modernos, pode aparecer como "Image-to-Text" ou "Document Question Answering" quando aplicado a documentos estruturados.
- **Aplica√ß√µes t√≠picas:** Leitura autom√°tica de notas fiscais, digitaliza√ß√£o de livros, extra√ß√£o de texto de placas, automa√ß√£o de formul√°rios.

---

## **32. Reconhecimento de Conex√µes (em diagramas, fluxogramas, etc.)**
- **Categoria:** Computer Vision > Image Feature Extraction / Object Detection / Keypoint Detection
- **Contexto:** O reconhecimento de conex√µes envolve identificar rela√ß√µes visuais entre elementos em uma imagem, como setas, linhas, conectores em diagramas ou fluxogramas. Pode envolver detec√ß√£o de objetos (setas/linhas), extra√ß√£o de caracter√≠sticas (features) ou pontos-chave (keypoints). Em contextos avan√ßados, pode ser chamado de "Graph Extraction".
- **Aplica√ß√µes t√≠picas:** Digitaliza√ß√£o de fluxogramas, reconstru√ß√£o de diagramas, an√°lise de mapas mentais, automa√ß√£o de engenharia e arquitetura.

---

## **33. Op√ß√µes de Carregamento e Otimiza√ß√£o de Modelos (textgen-webui)**

### Flags de Otimiza√ß√£o
- **load-in-8bit:** Carrega o modelo usando quantiza√ß√£o de 8 bits. Reduz uso de RAM/VRAM, com pequena perda de precis√£o. Ideal para placas com pouca mem√≥ria.
- **load-in-4bit:** Quantiza√ß√£o ainda mais agressiva (4 bits). Usa menos mem√≥ria, mas pode perder mais precis√£o. √ötil para modelos grandes em hardware limitado.
- **torch-compile:** Usa `torch.compile` (PyTorch 2.0+) para tentar acelerar a infer√™ncia. Pode melhorar performance em algumas GPUs/CPUs.
- **use_flash_attention_2:** Ativa uma implementa√ß√£o otimizada de aten√ß√£o (se suportado pelo modelo/hardware). Pode acelerar modelos Llama e similares.
- **use_double_quant:** Otimiza√ß√£o extra para quantiza√ß√£o 4bit. Usado junto com `load-in-4bit` para reduzir ainda mais o uso de mem√≥ria.
- **trust-remote-code:** Permite rodar c√≥digo customizado do reposit√≥rio do modelo (necess√°rio para alguns modelos do Hugging Face). Use apenas se confiar na fonte.

**Como usar:**
- Pela interface web: selecione as op√ß√µes ao carregar o modelo.
- Pela linha de comando ou `CMD_FLAGS.txt`:
  ```
  --load-in-8bit --use_flash_attention_2 --torch-compile
  ```

---

## **34. Model Loader (textgen-webui)**

O **Model Loader** √© o mecanismo que o textgen-webui usa para carregar diferentes formatos de modelo. A escolha correta garante compatibilidade e performance.

### Loaders dispon√≠veis:
- **transformers:** Para modelos Hugging Face (`.bin`, `.safetensors`). Suporta ampla variedade de arquiteturas.
- **exllama / exllama2:** Otimizados para modelos Llama em `.safetensors`. Mais r√°pido e eficiente para Llama-family.
- **llama.cpp:** Para modelos `.gguf` (quantizados, ideais para CPU/edge ou uso leve de GPU).
- **ggml:** Para modelos `.ggml` (formato antigo, menos usado atualmente).
- **Auto:** O webui tenta escolher automaticamente o loader mais adequado (funciona na maioria dos casos).

### Como escolher o loader:
- Se o modelo √© `.gguf` ‚Üí **llama.cpp**
- Se √© `.safetensors` de Llama ‚Üí **exllama** ou **exllama2**
- Se √© `.bin` ou `.safetensors` Hugging Face (n√£o-Llama) ‚Üí **transformers**
- Se n√£o sabe, tente **Auto** primeiro

**Como definir:**
- Na interface web: selecione o loader ao carregar o modelo.
- No `CMD_FLAGS.txt` ou linha de comando:
  ```
  --loader exllama
  --loader transformers
  --loader llama.cpp
  ```

**Dica:**
- Modelos Llama grandes rodam melhor com exllama/exllama2.
- Modelos quantizados para CPU (gguf) rodam melhor com llama.cpp.
- Modelos Hugging Face gen√©ricos use transformers.

---

## **35. Orquestra√ß√£o de Modelos em Ambientes Distribu√≠dos (Slice/ALIVE)**

### Onde o modelo roda: m√°quina do modelo ou m√°quina da execu√ß√£o?
- O modelo **sempre roda na m√°quina onde o servidor (ex: textgen-webui, Ollama, LiteLLM) est√° em execu√ß√£o**.
- O processamento (CPU/GPU, RAM) ocorre na m√°quina que executa o servi√ßo, independentemente de onde o arquivo do modelo est√° armazenado.
- Exemplo: Se o textgen-webui est√° rodando na localcloud, todo o processamento √© feito l√°, mesmo que voc√™ acesse da workstation.

### Comportamento com NFS/mount points
- Se voc√™ monta `/media/data/models` da localcloud na workstation via NFS, os arquivos de modelo ficam fisicamente na localcloud, mas aparecem como locais na workstation.
- Se rodar o servidor na workstation, ele l√™ os arquivos via NFS, mas o processamento √© feito na workstation.
- O acesso ao modelo pode ser mais lento (lat√™ncia de rede, I/O), mas o uso de hardware √© sempre da m√°quina que executa o servi√ßo.

### Carregamento, limpeza e escalabilidade de modelos no textgen-webui
- **Baixar modelo:** Baixe modelos via interface web, script ou manualmente para a pasta de modelos.
- **Carregar modelo:** S√≥ √© carregado na RAM/VRAM quando selecionado e carregado via interface ou API.
- **Limpar modelo:** Ao "unload" (descarrregar) um modelo, libera RAM/VRAM. Voc√™ pode deletar arquivos do disco a qualquer momento (desde que n√£o estejam carregados).
- **V√°rios modelos:** O textgen-webui permite v√°rios modelos no disco, mas **s√≥ carrega um por vez na mem√≥ria** (igual ao Ollama).
- **Escalar:** Para m√∫ltiplos modelos ativos, rode m√∫ltiplas inst√¢ncias do webui, cada uma em uma porta diferente, cada uma com um modelo carregado.

### Comparativo: textgen-webui vs Ollama
- **Ollama:** S√≥ 1 modelo carregado por vez por inst√¢ncia. Troca de modelo = descarrega um, carrega outro.
- **textgen-webui:** Tamb√©m s√≥ 1 modelo carregado por inst√¢ncia. Permite m√∫ltiplos modelos no disco, mas s√≥ 1 ativo. Para m√∫ltiplos modelos ativos, rode m√∫ltiplas inst√¢ncias.

### Dicas pr√°ticas para ambientes Slice/ALIVE
- **/media/data** √© o storage de produ√ß√£o e compartilhamento.
- Cada m√°quina pode rodar seu pr√≥prio webui, usando modelos locais ou via NFS.
- O processamento √© sempre na m√°quina que executa o servi√ßo, n√£o onde o arquivo est√° fisicamente.
- Para liberar espa√ßo: delete modelos n√£o usados do disco, use scripts para mover modelos entre m√°quinas conforme demanda.
- Para escalar: rode m√∫ltiplas inst√¢ncias do webui, cada uma com um modelo diferente, ou distribua a carga entre localcloud e workstation.
- Prefira rodar modelos grandes na localcloud (mais threads, sem interface gr√°fica, menos disputa de RAM).
- Use a workstation para prototipa√ß√£o, testes r√°pidos, ou modelos que exigem GPU NVIDIA.
- Use NFS para compartilhar modelos, mas lembre-se que o I/O pode ser gargalo para modelos muito grandes.
- Automatize scripts para mover/carregar/limpar modelos conforme o ciclo de uso.

### Resumo pr√°tico
- O modelo roda na m√°quina que executa o servidor, n√£o onde est√° armazenado.
- NFS permite compartilhar modelos, mas processamento √© sempre local ao servi√ßo.
- textgen-webui e Ollama s√≥ carregam 1 modelo por inst√¢ncia; para m√∫ltiplos, rode m√∫ltiplas inst√¢ncias.
- Gerencie espa√ßo e escalabilidade com scripts e automa√ß√£o.

---

## **36. Compatibilidade de Modelos com Hardware e Frameworks**

### Resumo Visual de Compatibilidade

| Formato/Framework      | CPU | GPU NVIDIA | GPU AMD | Observa√ß√£o                        |
|------------------------|-----|------------|---------|-----------------------------------|
| .gguf / llama.cpp      | üëç  | üëç         | üëç*     | *AMD/Apple: suporte parcial       |
| .safetensors / .bin    | üëç* | üëç         | üëé*     | *CPU lento, AMD s√≥ com ROCm       |
| Diffusion/StableDiff.  | üëé  | üëç         | üëé*     | *AMD s√≥ ROCm, inst√°vel            |
| transformers (HF)      | üëç* | üëç         | üëé*     | *CPU lento, AMD s√≥ ROCm           |

### Checklist para An√°lise de Compatibilidade (Hugging Face)

1. **Verifique a se√ß√£o "Hardware requirements" ou "Usage"**
   - Procure por men√ß√µes a CUDA, NVIDIA, ROCm, CPU, etc.
2. **Formato do modelo**
   - `.gguf`/`.ggml` ‚Üí CPU/edge, multiplataforma
   - `.safetensors`/`.bin` ‚Üí geralmente GPU NVIDIA
3. **Tags e Metadados**
   - Veja tags como `cuda`, `nvidia`, `rocm`, `llama.cpp`, `gguf`, etc.
4. **Framework usado**
   - PyTorch: NVIDIA (CUDA), ROCm (AMD, experimental)
   - llama.cpp: CPU, suporte parcial a AMD/Apple
5. **Exemplos de c√≥digo**
   - `.to('cuda')` ‚Üí espera GPU NVIDIA
   - `.to('cpu')` ‚Üí pode rodar em CPU (lento para modelos grandes)
6. **Tamanho do modelo**
   - Modelos grandes (>7B) sem quantiza√ß√£o: s√≥ vi√°veis em GPU potente
   - Modelos quantizados: vi√°veis em CPU, mais lentos

### Diagrama Visual de Decis√£o (Mermaid)

```mermaid
graph TD
    A[Modelo no Hugging Face] --> B{Formato do modelo}
    B -- .gguf/.ggml --> C[Use llama.cpp<br>CPU/edge, multiplataforma]
    B -- .safetensors/.bin --> D{Framework}
    D -- transformers/PyTorch --> E{Hardware}
    E -- NVIDIA dispon√≠vel --> F[Use GPU NVIDIA]
    E -- AMD dispon√≠vel --> G{ROCm suportado?}
    G -- Sim --> H[Use ROCm (experimental)]
    G -- N√£o --> I[Use CPU (lento)]
    E -- S√≥ CPU --> I[Use CPU (lento)]
    D -- llama.cpp --> C
    B -- Outro formato --> J[Verifique documenta√ß√£o]
```

### Dicas r√°pidas
- Prefira NVIDIA para m√°xima performance.
- Use .gguf/llama.cpp para portabilidade e baixo recurso.
- AMD s√≥ √© vi√°vel com ROCm (experimental) ou via llama.cpp.
- Sempre consulte exemplos e tags do modelo.

---

## **Dicas Gerais**
- Sempre consulte a documenta√ß√£o oficial do modelo para detalhes e limita√ß√µes.
- Sufixos e nomes podem variar entre projetos, mas os conceitos acima s√£o amplamente adotados.
- Para agentes Slice/ALIVE, prefira modelos instruct/chat para automa√ß√£o e orquestra√ß√£o, vision para multimodalidade, e embeddings multilingual para mem√≥ria vetorial.

---

## **37. Comparativo Pr√°tico de Fam√≠lias de Modelos LLM para C√≥digo**

### Llama (Meta)
- **O que √©:** Modelo base, generalista, multilingue, open source.
- **Pontos fortes:** Gera√ß√£o de texto, instru√ß√£o, chat, RAG, boa base para fine-tuning.
- **Para c√≥digo:** N√£o √© especializado, mas responde bem a prompts simples de c√≥digo.
- **Limita√ß√µes:** Pode confundir sintaxes, n√£o √© "code first", n√£o entende contexto de projetos complexos.
- **Quando usar:** Chatbots, agentes generalistas, prototipa√ß√£o, automa√ß√£o leve.

---

### CodeLlama (Meta)
- **O que √©:** Variante do Llama treinada especificamente para programa√ß√£o.
- **Pontos fortes:** Gera√ß√£o de c√≥digo, explica√ß√£o, refatora√ß√£o, suporte a m√∫ltiplas linguagens (Python, C, C++, JavaScript, TypeScript, Bash, etc.).
- **Para c√≥digo:** Muito melhor que Llama puro. Tem variantes para "Instruct" (segue comandos), "Python" (foco em Python), "Code" (geral), "Code Instruct" (segue instru√ß√µes de c√≥digo).
- **Limita√ß√µes:** Pode misturar sintaxes se o prompt for amb√≠guo, nem sempre entende contexto de frameworks modernos.
- **Quando usar:** Assistentes de c√≥digo, automa√ß√£o de refatora√ß√£o, gera√ß√£o de snippets, explica√ß√£o de c√≥digo.

---

### Mixtral (Mistral/Mixtral)
- **O que √©:** Modelo MoE (Mixture of Experts), open source, multilingue, variantes 8x7B, 8x22B, etc.
- **Pontos fortes:** Muito bom em instru√ß√£o, reasoning, chat, RAG, e razo√°vel para c√≥digo (mas n√£o √© "code first").
- **Para c√≥digo:** Gera c√≥digo, mas n√£o √© t√£o preciso quanto CodeLlama ou modelos dedicados. Melhor para prompts mistos (texto + c√≥digo).
- **Limita√ß√µes:** Pode errar sintaxe, n√£o √© especialista em nenhuma linguagem.
- **Quando usar:** Agentes generalistas, chatbots que precisam misturar texto e c√≥digo, automa√ß√£o com reasoning.

---

### WizardLM / WizardCoder / WizardLM2
- **O que √©:** Modelos baseados em Llama/Mistral, ajustados com dados de instru√ß√£o e c√≥digo (WizardCoder √© o "code first").
- **Pontos fortes:** Muito bons em seguir instru√ß√µes, explica√ß√£o de c√≥digo, gera√ß√£o de c√≥digo em m√∫ltiplas linguagens.
- **Para c√≥digo:** WizardCoder √© excelente para Python, razo√°vel para JS/TS, bom para explica√ß√£o e refatora√ß√£o.
- **Limita√ß√µes:** Pode "alucinar" sintaxe, especialmente em linguagens menos populares. Alguns modelos tendem a preferir Python.
- **Quando usar:** Assistentes de c√≥digo, bots de explica√ß√£o, automa√ß√£o de tarefas t√©cnicas.

---

### Qwen/Qwen2/Qwen3 (Alibaba)
- **O que √©:** Modelos multilingues, variantes "Code" e "Instruct", open weights, muito bons em reasoning e tasks complexas.
- **Pontos fortes:** √ìtimos em instru√ß√£o, reasoning, gera√ß√£o de c√≥digo, multilinguismo, variantes "Code" s√£o especializadas em programa√ß√£o.
- **Para c√≥digo:** Qwen2/Qwen3-Code s√£o muito bons para Python, JS, Java, C, etc. Costumam ser mais "assertivos" em Node.js do que Llama/CodeLlama.
- **Limita√ß√µes:** Documenta√ß√£o menos extensa, comunidade menor que Llama, pode ter vi√©s para Python em prompts gen√©ricos.
- **Quando usar:** Agentes t√©cnicos, automa√ß√£o de c√≥digo, tasks multilingues, integra√ß√£o com sistemas asi√°ticos.

---

### Outros modelos relevantes para c√≥digo

#### DeepSeek-Coder
- Foco total em programa√ß√£o, excelente para m√∫ltiplas linguagens, muito bom em completude e explica√ß√£o.

#### StarCoder / StarCoder2
- Foco em c√≥digo, especialmente bom para Python, JS, Java, C, SQL. √ìtimo para completude e explica√ß√£o.

#### Phind-CodeLlama
- Variante do CodeLlama ajustada para busca e explica√ß√£o, excelente para "search + code".

---

### Dicas pr√°ticas para escolher modelo de c√≥digo
- **Quer assertividade em Node.js/JS/TS?** Qwen2-Code, DeepSeek-Coder, StarCoder2 s√£o mais "assertivos" que CodeLlama puro.
- **Quer explica√ß√£o/refatora√ß√£o?** WizardCoder, CodeLlama-Instruct, StarCoder2.
- **Quer completude e snippets?** DeepSeek-Coder, StarCoder2, CodeLlama.
- **Quer multilinguismo?** Qwen3, Mixtral, DeepSeek-Coder.
- **Quer reasoning + c√≥digo?** Mixtral, Qwen3, WizardLM2.

---

### Aten√ß√£o!
- Modelos "code first" tendem a preferir Python se o prompt for amb√≠guo ("escreva uma fun√ß√£o que..."). Sempre especifique a linguagem: "em JavaScript", "em TypeScript", etc.
- Alguns modelos podem misturar sintaxes se o contexto n√£o estiver claro.
- Teste sempre com exemplos reais do seu stack.

---

### Tabela-Resumo para Consulta R√°pida

| Modelo/Fam√≠lia      | Code First | Melhor em...         | Limita√ß√µes                | Linguagens fortes      |
|---------------------|------------|----------------------|---------------------------|-----------------------|
| Llama               | N√£o        | Texto, instru√ß√£o     | N√£o √© especialista        | Multilingue           |
| CodeLlama           | Sim        | Gera√ß√£o, explica√ß√£o  | Pode misturar sintaxes    | Python, JS, Bash      |
| Mixtral             | Parcial    | Reasoning, instru√ß√£o | N√£o √© code first          | Multilingue           |
| WizardCoder         | Sim        | Explica√ß√£o, refator  | Vi√©s para Python          | Python, JS, C         |
| Qwen2/Qwen3-Code    | Sim        | Assertividade, multi | Documenta√ß√£o menor        | Python, JS, Java      |
| DeepSeek-Coder      | Sim        | Completude, multi    | Comunidade menor          | Python, JS, C, Java   |
| StarCoder2          | Sim        | Completude, explic.  | Vi√©s para Python/JS       | Python, JS, SQL, C    |

---

> Atualize este guia conforme surgirem novos termos ou modelos relevantes para o ecossistema.
>
## 2024-06-XX ‚Äî Modelos LLM que Afinam Melhor com Node.js (e menos "pythonzeira")

- [x] Observa√ß√£o pr√°tica: Muitos modelos LLM open source (e at√© closed) t√™m vi√©s forte para Python, especialmente quando o prompt √© amb√≠guo ou pede "escreva uma fun√ß√£o" sem especificar linguagem.
- [x] Dica: Sempre especifique  JavaScript" ou "em TypeScript" no prompt para evitar que o modelo gere c√≥digo Python por padr√£o.
- [x] Modelos que afinam melhor com Node.js/JS/TS:
  - **Qwen2-Code, Qwen3-Code:** Costumam ser mais assertivos em JS/TS, menos vi√©s Python, bons para automa√ß√£o Node.js.
  - **DeepSeek-Coder:** Muito bom para m√∫ltiplas linguagens, assertivo em Node.js, gera c√≥digo moderno e idiom√°tico.
  - **StarCoder2:** √ìtimo para JS/TS, SQL, Python, mas responde bem a prompts claros para Node.js.
  - **CodeLlama-Instruct:** Bom para JS/TS, mas pode "pythonizar" se o prompt for vago.
  - **Phind-CodeLlama:** Foco em busca + c√≥digo, responde bem a prompts de automa√ß√£o, mas ainda pode preferir Python se n√£o for expl√≠cito.
- [x] Modelos que "pythonizam" tudo:
  - **WizardCoder, WizardLM2:** Excelentes para Python, mas tendem a gerar Python at√© quando o prompt pede JS, se n√£o for muito claro.
  - **Llama base:** N√£o √© code first, mas se pedir c√≥digo, geralmente gera Python.
- [x] Dica de ouro: Para automa√ß√£o Node.js, sempre inclua exemplos, contexto de uso ("em um projeto Node.js", "usando fs/promises", "com async/await", etc.) e pe√ßa testes em JS/TS.
- [x] Moral: Aqui √© Node.js raiz, mas a IA sempre tenta "pythonizar" se der brecha. Prompt claro = c√≥digo certo!

## 38. Hiperpar√¢metros em Fine-Tuning de LLMs: Conceitos, Exemplos e Estrat√©gias

### O que s√£o hiperpar√¢metros?
- Hiperpar√¢metros s√£o "configura√ß√µes de controle" do processo de treinamento de um modelo. Eles determinam **como** o modelo aprende, mas n√£o s√£o aprendidos pelo modelo ‚Äî voc√™ define antes de treinar.
- Ajustar hiperpar√¢metros √© essencial para garantir que o modelo aprenda de forma eficiente, sem overfitting (decorar o dataset) ou underfitting (n√£o aprender nada √∫til).

---

### Exemplos de hiperpar√¢metros comuns em fine-tuning de LLMs

- **learning_rate (taxa de aprendizado):**
  - Controla o tamanho dos passos que o modelo d√° ao ajustar seus pesos.
  - **Baixo:** Aprendizado mais lento, menos risco de instabilidade, pode exigir mais epochs.
  - **Alto:** Aprendizado r√°pido, mas pode "pular" √≥timas solu√ß√µes ou ficar inst√°vel.
  - **Dica:** Comece com valores entre 1e-5 e 5e-5 para LLMs, ajuste conforme o comportamento.

- **batch_size (tamanho do lote):**
  - Quantos exemplos o modelo processa antes de atualizar os pesos.
  - **Pequeno:** Menos mem√≥ria, mais ru√≠do, pode ajudar a generalizar.
  - **Grande:** Mais est√°vel, mais r√°pido, mas exige mais RAM/VRAM.
  - **Dica:** Use o maior batch_size que seu hardware permitir sem travar.

- **num_epochs (n√∫mero de √©pocas):**
  - Quantas vezes o modelo passa por todo o dataset.
  - **Poucas epochs:** Menos risco de overfitting, pode n√£o aprender tudo.
  - **Muitas epochs:** Aprende mais, mas pode "decorar" o dataset.
  - **Dica:** Para datasets pequenos, use mais epochs; para grandes, menos.

- **weight_decay (decaimento de peso):**
  - Penaliza pesos grandes para evitar overfitting.
  - **Alto:** Mais regulariza√ß√£o, menos overfitting, mas pode limitar aprendizado.
  - **Baixo/zero:** Modelo pode "decorar" exemplos.
  - **Dica:** Valores t√≠picos: 0.01 a 0.1.

- **warmup_steps:**
  - N√∫mero de passos iniciais em que o learning_rate sobe gradualmente at√© o valor final.
  - Ajuda a estabilizar o in√≠cio do treinamento.
  - **Dica:** 5% a 10% do total de steps √© comum.

- **max_seq_length (comprimento m√°ximo da sequ√™ncia):**
  - Quantos tokens o modelo processa de uma vez.
  - **Curto:** Menos mem√≥ria, pode truncar exemplos longos.
  - **Longo:** Mais contexto, mais mem√≥ria.
  - **Dica:** Ajuste conforme o tamanho t√≠pico dos seus exemplos.

- **gradient_accumulation_steps:**
  - Acumula gradientes de v√°rios batches antes de atualizar os pesos.
  - Permite simular batch_size maior em hardware limitado.
  - **Dica:** √ötil quando batch_size √© pequeno por limita√ß√£o de RAM/VRAM.

---

### Por que ajustar hiperpar√¢metros? Estrat√©gias e Resultados

- **Cada dataset e objetivo √© √∫nico:**
  - Datasets pequenos exigem cuidado extra para n√£o "decorar" exemplos (overfitting). Use learning_rate menor, mais epochs, weight_decay maior.
  - Datasets grandes permitem learning_rate maior, menos epochs, batch_size maior.

- **Hardware limita escolhas:**
  - Pouca RAM/VRAM? Use batch_size menor, gradient_accumulation_steps maior.
  - Hardware robusto? Experimente batch_size maior para acelerar.

- **Resultados esperados ao ajustar:**
  - **Learning_rate muito alto:** Modelo oscila, n√£o converge, outputs ruins.
  - **Learning_rate muito baixo:** Treinamento lento, pode n√£o sair do lugar.
  - **Batch_size muito pequeno:** Treinamento inst√°vel, outputs variam muito.
  - **Batch_size muito grande:** Treinamento est√°vel, mas pode "perder" nuances do dataset.
  - **Epochs demais:** Modelo come√ßa a "decorar" exemplos, perde capacidade de generalizar.
  - **Weight_decay baixo:** Overfitting, modelo repete exemplos do dataset.

- **Estrat√©gias pr√°ticas:**
  - Comece com hiperpar√¢metros padr√£o (ex: learning_rate=2e-5, batch_size=8, epochs=3).
  - Monitore m√©tricas de valida√ß√£o (loss, accuracy, perplexity, etc.).
  - Se o modelo n√£o melhora, ajuste learning_rate ou batch_size.
  - Se o modelo "decora" exemplos, aumente weight_decay ou reduza epochs.
  - Use early stopping (parar o treino se n√£o melhorar ap√≥s X passos).

- **Dica de ouro:**
  - Documente cada experimento, compare resultados e ajuste de forma incremental.
  - N√£o existe "receita m√°gica" ‚Äî ajuste √© sempre emp√≠rico e depende do seu contexto.

---
