# ğŸ›¡ï¸ Model Provider API Gateway (Super Ollama)

## ğŸ“ DescriÃ§Ã£o
Gateway centralizado e extensÃ­vel que provisiona, gerencia e expÃµe modelos diversos (LLMs, modelos de visÃ£o, etc.) de forma rÃ¡pida, padronizada e compatÃ­vel com a API do OpenAI. Atua como "ponte" e orquestrador entre mÃºltiplos backends (Ollama, HuggingFace, vLLM, text-generation-webui, etc.), abstraindo diferenÃ§as e centralizando operaÃ§Ãµes administrativas e de inferÃªncia.

## ğŸ“‹ Responsabilidades
- Instalar, mover, duplicar, deletar, ativar/desativar e listar modelos via API/CLI centralizada, inclusive em mÃºltiplas mÃ¡quinas.
- Gerenciar mÃºltiplas instÃ¢ncias/versÃµes: permitir diferentes versÃµes/modelos disponÃ­veis simultaneamente, inclusive para testes A/B, staging e produÃ§Ã£o.
- Encapsular comandos e paths de cada backend, abstraindo detalhes de instalaÃ§Ã£o, armazenamento e execuÃ§Ã£o (ex: CLI do Ollama, scripts Python, containers, etc.).
- Gerenciar recursos multi-mÃ¡quina: decidir onde rodar cada modelo (CPU/GPU, mÃ¡quina 1 ou 2), balancear carga e otimizar uso de hardware.
- Expor endpoints REST padronizados: garantir compatibilidade total com clientes OpenAI (ex: chat/completions, embeddings, etc.).
- Permitir operaÃ§Ãµes administrativas (instalar, mover, deletar, ativar/desativar modelos) via API/CLI centralizada.
- Integrar mÃ³dulos plugÃ¡veis para fine-tuning (LoRA, etc.) e RAG (indexaÃ§Ã£o, busca, injeÃ§Ã£o de contexto).
- AutomaÃ§Ã£o e rapidez: permitir que novos modelos sejam adicionados, removidos ou atualizados rapidamente, com mÃ­nimo downtime.
- Observabilidade e logs: integrar com mÃ³dulos de rastreabilidade (ex: Opik) para logging, tracing, mÃ©tricas de uso, custos e incidentes.
- Interface de administraÃ§Ã£o: expor APIs ou UI para listar, ativar/desativar, atualizar e monitorar modelos disponÃ­veis.

## ğŸ› ï¸ PadrÃ£o de ImplementaÃ§Ã£o
- Backend em TypeScript/Node.js, com arquitetura modular e plugÃ¡vel para suportar mÃºltiplos providers e agentes remotos.
- AdaptaÃ§Ã£o de payloads e respostas para garantir compatibilidade OpenAI.
- Scripts e automaÃ§Ãµes para instalaÃ§Ã£o rÃ¡pida de modelos (ex: download, setup, healthcheck), inclusive via CLI remota.
- IntegraÃ§Ã£o com sistemas de autenticaÃ§Ã£o, rate limiting, seguranÃ§a e automaÃ§Ã£o (CI/CD, scripts, etc.).
- Plugins/adapters para cada backend (Ollama, HuggingFace, vLLM, etc.), encapsulando comandos e paths.
- Banco de dados/registro central para saber onde estÃ¡ cada modelo, status, logs, etc.
- IntegraÃ§Ã£o plugÃ¡vel com mÃ³dulos de fine-tuning e RAG (pode usar libs Python via subprocess, containers, etc.).

## ğŸ§ª Exemplo oficial
- [Exemplo de provisionamento e exposiÃ§Ã£o de modelo HuggingFace via API OpenAI-like](@/examples/model-provider-hf-openai.ts)

## ğŸ’¡ ObservaÃ§Ãµes
- O mÃ³dulo nÃ£o gerencia ciclo de vida experimental (isso Ã© papel do ManicÃ´mio), mas sim o provisionamento, orquestraÃ§Ã£o e exposiÃ§Ã£o de modelos prontos para uso.
- Pode ser integrado com o RIA para ativaÃ§Ã£o/desativaÃ§Ã£o dinÃ¢mica de recursos IA conforme demanda do ecossistema.
- Permite automaÃ§Ã£o e integraÃ§Ã£o fÃ¡cil com scripts, CI/CD e outros mÃ³dulos do Slice/ALIVE.

---

## ğŸ§© Ferramentas de terceiros utilizadas e features

### ğŸ¦™ Ollama
- ğŸ”— RepositÃ³rio: https://github.com/ollama/ollama
- ğŸ“„ LicenÃ§a: https://github.com/ollama/ollama/blob/main/LICENSE
- ğŸš€ Feature: Backend LLM local, rÃ¡pido, privado, integrÃ¡vel com outros frameworks. Permite deploy e gerenciamento de modelos via CLI/API.

### ğŸŒ text-generation-webui
- ğŸ”— RepositÃ³rio: https://github.com/oobabooga/text-generation-webui
- ğŸ“„ LicenÃ§a: https://github.com/oobabooga/text-generation-webui/blob/main/LICENSE
- ğŸš€ Feature: Interface web para deploy rÃ¡pido de modelos LLM, suporte a mÃºltiplos formatos, plugins de API (REST, OpenAI-compatible), gerenciamento visual de modelos.

### ğŸ”€ LiteLLM
- ğŸ”— RepositÃ³rio: https://github.com/BerriAI/litellm
- ğŸ“„ LicenÃ§a: https://github.com/BerriAI/litellm/blob/main/LICENSE
- ğŸš€ Feature: Proxy/servidor OpenAI-compatible, roteamento dinÃ¢mico de requisiÃ§Ãµes para mÃºltiplos backends, logging, mÃ©tricas e swap de modelos sem alterar cÃ³digo cliente.
