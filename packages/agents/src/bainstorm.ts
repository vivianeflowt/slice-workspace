import axios, { AxiosInstance } from 'axios';
import Bluebird from 'bluebird';
import type { AxiosError, AxiosRequestConfig } from 'axios';
import fs from 'fs';
import path from 'path';
import { getMaxResponseTokens } from './utils/tokens';
import { estimateTokenCount } from './utils/tokens';
import { sanitizeCodeString } from './utils/sanitizers';
import fg from 'fast-glob';

// === CONSTANTS ===
export const SYSTEM_PROMPT_TECNICO = `
## Diretriz Inicial

Voc√™ √© uma IA especialista em an√°lise sem√¢ntica, curadoria e extra√ß√£o de relev√¢ncia em textos complexos.
Sua miss√£o √© identificar, com precis√£o e discernimento, informa√ß√µes de alto valor em grandes volumes de dados, atuando como um agente de busca, filtro e classifica√ß√£o de conte√∫dos relevantes para o contexto de neg√≥cio.
Utilize sua capacidade de compreens√£o contextual, julgamento cr√≠tico e aten√ß√£o a detalhes para entregar respostas claras, objetivas e confi√°veis, sempre seguindo as melhores pr√°ticas de curadoria textual e an√°lise sem√¢ntica.

## Diretrizes de Comportamento

1. Voc√™ √© um agente de curadoria respons√°vel por analisar textos conforme instru√ß√µes espec√≠ficas.
2. Responda sempre de forma objetiva, sem explica√ß√µes extras.
3. Utilize o formato de resposta especificado abaixo.
4. Use marca√ß√£o markdown para estruturar sua resposta, conforme exemplos.
5. Sempre inclua o grau de confian√ßa na resposta, no formato: (confian√ßa: X.XX)

## Sobre o grau de confian√ßa

- O valor de confian√ßa deve refletir o quanto a evid√™ncia no texto √© clara, direta e inequ√≠voca.
- Use a escala de 0.0 (nenhuma certeza) a 1.0 (certeza absoluta).
- Em casos amb√≠guos, parciais ou de baixa evid√™ncia, use valores intermedi√°rios ou baixos (ex: 0.3, 0.5, 0.6).
- S√≥ use valores altos (ex: 0.9+) quando a informa√ß√£o for expl√≠cita, direta e sem margem para d√∫vida.
- Em caso de d√∫vida, seja conservador e reduza a confian√ßa.
- Mesmo que a resposta seja CONTEM, se a evid√™ncia for fraca, a confian√ßa deve ser baixa.

## Formato de Resposta

- Se o texto CONT√âM o contexto solicitado, responda:
  CONTEM (confian√ßa: X.XX):
  "Trecho 1"
  "Trecho 2"
- Se o texto N√ÉO CONT√âM, responda:
  NAO CONTEM (confian√ßa: X.XX)

## Exemplos

- CONTEM (confian√ßa: 0.35):
  "Trecho com men√ß√£o vaga ou indireta ao contexto solicitado."
- CONTEM (confian√ßa: 0.65):
  "Trecho que aborda parcialmente o contexto, mas com alguma incerteza."
- CONTEM (confian√ßa: 0.95):
  "Trecho expl√≠cito, direto e inequ√≠voco sobre o contexto solicitado."
- NAO CONTEM (confian√ßa: 0.5)
`;

export const SYSTEM_PROMPT_NEGOCIO = `
## Crit√©rios para Identifica√ß√£o de Informa√ß√µes de Neg√≥cio

Considere que o texto CONT√âM informa√ß√µes de neg√≥cio apenas se houver men√ß√£o expl√≠cita a:
- Cliente
- Valor monet√°rio
- Lucro
- Receita
- Investimento
- Proposta de valor
- Modelo de neg√≥cio
- Monetiza√ß√£o
- Vendas
- Contratos
- Impacto financeiro direto
- Funcionamento do ecossistema Slice/ALIVE enquanto neg√≥cio

## Exemplos Positivos (CONTEM)

- "Ap√≥s a integra√ß√£o do Slice/ALIVE, o cliente X conseguiu automatizar fluxos e reportou aumento de receita no trimestre."
- "O contrato firmado com a empresa Y prev√™ suporte dedicado e pagamento mensal recorrente pelo uso do ecossistema."
- "A proposta de valor do Slice/ALIVE √© acelerar a entrega de resultados financeiros para nossos clientes por meio de automa√ß√£o inteligente."
- "A ado√ß√£o do pipeline de IA local permitiu ao cliente Z economizar R$ 15 mil mensais em custos operacionais."
- "O investimento realizado na infraestrutura de IA foi recuperado em seis meses devido ao aumento de vendas e redu√ß√£o de despesas."
- "Nosso modelo de neg√≥cio prev√™ monetiza√ß√£o por assinatura, com planos diferenciados para pequenas e grandes empresas."
- "A automa√ß√£o de experimentos proporcionou ao cliente ganhos de produtividade e impacto financeiro direto no resultado do neg√≥cio."
- "A venda do m√≥dulo de an√°lise sem√¢ntica gerou lucro l√≠quido de R$ 8 mil no primeiro m√™s de opera√ß√£o."
- "O funcionamento do ecossistema Slice/ALIVE permite integra√ß√£o r√°pida com clientes corporativos, acelerando o ciclo de vendas."
- "A receita do √∫ltimo trimestre cresceu 15% ap√≥s a implementa√ß√£o do novo fluxo de automa√ß√£o para clientes estrat√©gicos."

## Exemplos Negativos (NAO CONTEM)

- "A instala√ß√£o dos modelos Ollama foi conclu√≠da com sucesso no ambiente local."
- "O sistema utiliza arquitetura de microservi√ßos para garantir escalabilidade e modularidade."
- "Implementamos um novo algoritmo de tokeniza√ß√£o baseado em SentencePiece para otimizar o processamento."
- "O diret√≥rio lib concentra utilit√°rios centrais para o funcionamento interno do sistema."
- "A documenta√ß√£o foi atualizada para incluir exemplos de uso da API e instru√ß√µes de deploy."
- "O fluxo experimental do projeto ERD √© iterativo, rastre√°vel e otimizado continuamente."
- "A automa√ß√£o de experimentos permite registrar logs estruturados e analisar resultados t√©cnicos."
- "O MCP permite gerenciar, rodar e alternar entre m√∫ltiplos modelos de IA localmente."
- "A configura√ß√£o do ambiente MCP cobre automa√ß√£o, an√°lise e produtividade de desenvolvimento."
- "O conceito do diret√≥rio providers √© centralizar integra√ß√µes com servi√ßos externos, como bancos de dados e APIs."

## Exemplos de Falsos Positivos Frequentes (NAO CONTEM)

- "A economia de recursos computacionais foi poss√≠vel ao rodar modelos localmente." (economia t√©cnica, n√£o valor de neg√≥cio)
- "O contrato da fun√ß√£o define os tipos de entrada e sa√≠da no c√≥digo." (contrato t√©cnico, n√£o contrato de neg√≥cio)
- "A receita do comando √© executar o script build.sh antes do deploy." (receita t√©cnica, n√£o financeira)
- "O modelo de dados foi atualizado para suportar m√∫ltiplos clientes simult√¢neos." (cliente como entidade t√©cnica, n√£o cliente de neg√≥cio)
- "O investimento de tempo em testes automatizados reduziu bugs em produ√ß√£o." (investimento de esfor√ßo, n√£o capital financeiro)
- "Sugest√£o de instala√ß√£o do Ollama para j√° deixar pronto o ambiente de IA." (instala√ß√£o t√©cnica, n√£o decis√£o de neg√≥cio)
- "LangChain pode ser avaliado e integrado depois caso o projeto evolua para fluxos de IA mais complexos." (planejamento t√©cnico, n√£o decis√£o de neg√≥cio)
- "Voc√™ est√° com um ecossistema extremamente completo e pronto para automa√ß√£o, an√°lise e produtividade de desenvolvimento." (elogio t√©cnico, n√£o resultado de neg√≥cio)
- "Fun√ß√£o permite gerenciar, rodar, listar, deletar e interagir com modelos Ollama via MCP." (descri√ß√£o t√©cnica de ferramenta, n√£o informa√ß√£o de neg√≥cio)
- "O fluxo do projeto prev√™ logs detalhados para cada experimento realizado." (organiza√ß√£o experimental, n√£o informa√ß√£o de neg√≥cio)
`;

export const SYSTEM_PROMPT = SYSTEM_PROMPT_TECNICO + '\n' + SYSTEM_PROMPT_NEGOCIO;

export const BASE_USER_PROMPT = `
O texto CONTEM ou NAO CONTEM informa√ß√µes de neg√≥cio/cliente/valor/lucro/ecossistema? Se CONTEM, mostre o(s) trecho(s) exato(s).
`;

export const RESULT_FILE_PATH = path.join(__dirname, '..', 'tmp', 'BRAINSTORM-RESULTS.md'); // Nunca sobrescrever, s√≥ append!

export const API_BASE_URL = 'http://localhost:4000/api';
export const IA_CHAT_COMPLETIONS_ENDPOINT = '/chat/completions';
export const DEFAULT_TIMEOUT = 10 * 60 * 1000; // 10 minutes
export const DEFAULT_RETRY = 3;
export const IA_TEMPERATURE = 0.3;
export const ID_MAX_TOKENS_PER_CHUNK = 8000;
export const IA_MIN_TOKEN = 1024;
export const IA_CONTEXT_RATIO = 2.5;

export const providerModelOptions: { provider: string; model: string }[] = [
  { provider: 'ollama', model: 'llama3.1:8b' },
//   { provider: 'ollama', model: 'mistral:7b-instruct' },
//   { provider: 'ollama', model: 'qwen3:8b' },
//   { provider: 'ollama', model: 'deepseek-r1:8b' },
//   { provider: 'ollama', model: 'phi4:14b' },
  // { provider: 'ollama', model: 'neural-chat:7b' },
  // { provider: 'ollama', model: 'dolphin-mistral:7b' },
  // { provider: 'ollama', model: 'llama3:8b' },
];

// Arquivos do stack textgen-webui para brainstorm incremental
export const textgenStackFileList: string[] = [
  //   path.join('.docker', 'workstation', 'Dockerfile'),
  //   path.join('.docker', 'workstation', 'docker-compose.yml'),
  //   path.join('.docker', 'workstation', 'start.sh'),
  //   path.join('.docker', 'workstation', 'settings.yaml'),
  //   path.join('BRAINSTORM-RESULTS-PROCESSED.md'), // descomente se quiser incluir
  // --- SPLIT: partes para processamento incremental manual ---
  //   path.join('data', 'ArquivoSIM.md'),
  //   path.join('data', 'ArquivoNAO.md'),
];

// Diret√≥rio dos chunks para processamento incremental
export const CHUNKS_DIR_PATH = path.join(__dirname, '..', 'tmp', 'chunks');

// Custom Axios instance
export function createIAInstance({
  baseURL = API_BASE_URL,
  timeout = DEFAULT_TIMEOUT,
} = {}): AxiosInstance {
  const instance = axios.create({ baseURL, timeout });
  instance.interceptors.response.use(undefined, async (error: AxiosError) => {
    const requestConfig = error.config as AxiosRequestConfig & { __retryCount?: number };
    if (!requestConfig || (requestConfig.__retryCount ?? 0) >= DEFAULT_RETRY) throw error;
    requestConfig.__retryCount = (requestConfig.__retryCount ?? 0) + 1;
    return instance(requestConfig);
  });
  return instance;
}

export const iaAxios = createIAInstance();

// Log incremental de resultado
export const appendBrainstormResult = ({
  provider,
  model,
  file,
  result,
  temperature = IA_TEMPERATURE,
  files = [],
  maxTokens,
  promptTokens,
  resultLength,
}: {
  provider: string;
  model: string;
  file: string;
  result: string;
  temperature?: number;
  files?: string[];
  maxTokens?: number;
  promptTokens?: number;
  resultLength?: number;
}): void => {
  const fileList = files.length > 0 ? files.join(', ') : file;
  const log = `\n---\nprovider: ${provider}\nmodel: ${model}\ntemperature: ${temperature}\npromptTokens: ${promptTokens ?? ''}\nmaxTokens: ${maxTokens ?? ''}\nresultLength: ${resultLength ?? ''}\nfiles: ${fileList}\n\n${result}\n\n---\n`;
  fs.appendFileSync(RESULT_FILE_PATH, log);
  console.log(log);
};

// Utilit√°rio para dividir conte√∫do em partes de at√© 8k tokens
const splitFileForPromptChunks = (
  name: string,
  content: string,
  maxTokensPerPart = ID_MAX_TOKENS_PER_CHUNK
): { role: 'user'; content: string }[] => {
  const sanitized = sanitizeCodeString(content);
  const parts: { role: 'user'; content: string }[] = [];
  let start = 0;
  const approxChunkSize = Math.ceil(maxTokensPerPart / 0.24);
  while (start < sanitized.length) {
    let end = Math.min(start + approxChunkSize, sanitized.length);
    if (end < sanitized.length) {
      const nextNewline = sanitized.indexOf('\n', end);
      if (nextNewline !== -1 && nextNewline - end < 200) {
        end = nextNewline + 1;
      }
    }
    const chunk = sanitized.slice(start, end);
    const tokens: number = (estimateTokenCount as (input: string) => number)(chunk);
    if (tokens > maxTokensPerPart) {
      end = start + Math.floor(chunk.length * (maxTokensPerPart / tokens));
    }
    const finalChunk = sanitized.slice(start, end);
    parts.push({
      role: 'user',
      content: `File: ${name} (part ${parts.length + 1})\n\n${BASE_USER_PROMPT}\n\n${finalChunk}`,
    });
    start = end;
  }
  return parts;
};

export const brainstormFileList: { name: string; content: string }[] = [];

// Fun√ß√£o utilit√°ria para calcular o maxTokens ajustado
function calculateMaxTokens(baseTokens: number): number {
  let maxTokens = Math.ceil(baseTokens * IA_CONTEXT_RATIO);
  maxTokens = Math.max(IA_MIN_TOKEN, maxTokens);
  maxTokens = Math.ceil(maxTokens / 8) * 8;
  return maxTokens;
}

// Cria payload IA
export function createBrainstormPayload({
  files = brainstormFileList,
  systemPrompt = SYSTEM_PROMPT,
  provider,
  model,
  temperature = IA_TEMPERATURE,
}: {
  files?: { name: string; content: string }[];
  systemPrompt?: string;
  provider: string;
  model: string;
  temperature?: number;
}) {
  interface IAMessage {
    role: 'system' | 'user' | 'assistant';
    content: string;
  }
  interface IAPayload {
    provider: string;
    model: string;
    messages: IAMessage[];
    temperature?: number;
    maxTokens?: number;
    _promptTokens?: number;
  }
  const messages: IAMessage[] = [{ role: 'system', content: systemPrompt }];
  for (const file of files) {
    messages.push(...splitFileForPromptChunks(file.name, file.content, ID_MAX_TOKENS_PER_CHUNK));
  }
  const promptText: string = messages.map((message: IAMessage) => message.content).join('\n');

  let maxTokens: number = getMaxResponseTokens(promptText, '', undefined);
  // Limite m√°ximo do modelo (ex: 128k tokens)

  maxTokens = calculateMaxTokens(maxTokens);

  const payload: IAPayload = {
    provider,
    model,
    messages,
    temperature,
    maxTokens,
  };
  return payload;
}

// Tornar brainstormFilesMulti robusto: sempre continua mesmo com erro em algum modelo
async function brainstormFilesMulti({
  files,
  providersModels,
  systemPrompt,
  temperature,
}: {
  files: { name: string; content: string }[];
  providersModels: { provider: string; model: string }[];
  systemPrompt: string;
  temperature: number;
}): Promise<void> {
  for (const { provider, model } of providersModels) {
    const payload = createBrainstormPayload({
      files,
      systemPrompt,
      provider,
      model,
      temperature,
    });

    const response = await iaAxios.post<{
      choices?: { message?: { content?: string } }[];
      result?: string;
    }>(IA_CHAT_COMPLETIONS_ENDPOINT, payload);
    const data = response.data;
    let brainstormResult: string;
    if (Array.isArray(data?.choices) && data.choices[0]?.message?.content) {
      brainstormResult = data.choices[0].message.content;
    } else if (typeof data?.result === 'string') {
      brainstormResult = data.result;
    } else {
      brainstormResult = JSON.stringify(data);
    }
    // Valida√ß√£o bruta dos campos YAML obrigat√≥rios
    // Log de rastreabilidade: tamanho do campo result recebido do servidor
    if (typeof data?.result === 'string') {
      console.log(`[DEBUG] Tamanho do campo result recebido do servidor:`, data.result.length);
    }
    // Log de rastreabilidade: tamanho do campo registrado no arquivo
    console.log(`[DEBUG] Tamanho do campo registrado no arquivo:`, brainstormResult.length);
    appendBrainstormResult({
      provider,
      model,
      file: files.map((f) => f.name).join(', '),
      result: brainstormResult, // Passa o resultado integral, sem cortes
      temperature,
      files: files.map((f) => f.name),
      maxTokens: payload.maxTokens,
      promptTokens: payload._promptTokens,
      resultLength: brainstormResult.length,
    });
  }
}

// === CONTENT CLEANING STEPS ===

function removeTabsStep(linha: string): string {
  return linha.replace(/\t/g, '');
}

function removeNewLinesStep(linha: string): string {
  return linha.replace(/\n/g, '');
}

function normalizeSpacesStep(linha: string): string {
  return linha.replace(/\s+/g, ' ');
}

function trimStep(linha: string): string {
  return linha.trim();
}

function removeEmojisStep(linha: string): string {
  return linha.replace(/[\u{1F600}-\u{1F64F}]/gu, '');
}

function removeSpecialCharsStep(linha: string): string {
  return linha.replace(/[^\w\s\u00C0-\u017F]/g, '');
}

function isValidLineStep(linha: string): boolean {
  // Permite letras, n√∫meros, espa√ßos e acentos
  const hasContent = linha.length > 3;
  const hasValidChars = /^[\w\s\u00C0-\u017F]+$/.test(linha);
  return hasContent && hasValidChars;
}

function processContentLines(conteudo: string): string[] {
  return conteudo
    .split('\n')
    .map(removeTabsStep)
    .map(removeNewLinesStep)
    .map(normalizeSpacesStep)
    .map(trimStep)
    .map(removeEmojisStep)
    .map(removeSpecialCharsStep)
    .filter(isValidLineStep);
}

async function loadChunks() {
  const files = await fg(['*.md'], {
    onlyFiles: true,
    cwd: CHUNKS_DIR_PATH,
    absolute: true,
    suppressErrors: true,
  });

  const sortedFiles = files.sort((a, b) => a.localeCompare(b));

  return Bluebird.map(sortedFiles, async (file) => {
    const rawContent = await fs.promises.readFile(file, 'utf8');
    const cleanedLines = processContentLines(rawContent);

    // const businessMarks = [
    //   'investiu R$ 1,2 milh√£o',
    //   'cliente prefere solucoes plug and play',
    //   '√∫ltimo trimestre, dobrando o lucro bruto',
    //   'Compel investiu R$ 1,2 milh√£o',
    //   'valor de mercado',
    //   'lucro bruto cresceu 10%',
    //   'com essa margem de receita poremos evoluir nosso hardware a exponencial',
    //   'nosso ecositema garante que entregamos a solucao em 2 semanas',
    // ];

    // const shouldAddBusinessMark = Math.random() < 0.5;
    // if (shouldAddBusinessMark) {
    //   const randomMark = businessMarks[Math.floor(Math.random() * businessMarks.length)];
    //   cleanedLines.push(randomMark);
    // }

    // const fileExtension = shouldAddBusinessMark ? '.sim.md' : '.nao.md';
    const fileExtension = '.md';

    const newFileName = path.join(
      CHUNKS_DIR_PATH,
      path.basename(file).replace('.md', fileExtension)
    );
    const finalContent = cleanedLines.join('\n');

    await fs.promises.writeFile(newFileName, finalContent);

    return newFileName;
  });
}

const run = async () => {
  // Remove o arquivo de output se existir antes de come√ßar
  if (fs.existsSync(RESULT_FILE_PATH)) {
    fs.rmSync(RESULT_FILE_PATH);
  }

  if (!fs.existsSync(CHUNKS_DIR_PATH)) {
    fs.mkdirSync(CHUNKS_DIR_PATH, { recursive: true });
  }

  const chunkFiles = await loadChunks();

  chunkFiles.forEach((file) => {
    if (!textgenStackFileList.includes(file)) {
      textgenStackFileList.push(file);
    }
  });

  console.log(chunkFiles);

  try {
    console.log('üöÄ Starting multi-IA brainstorm for textgen-webui stack files...');
    // Deduplica arquivos por nome
    const fileMap = new Map<string, string>();
    for (const filePath of textgenStackFileList) {
      if (!fileMap.has(filePath)) {
        fileMap.set(
          filePath,
          fs.existsSync(filePath) ? fs.readFileSync(filePath, 'utf8') : '[File not found]'
        );
      }
    }
    const files = Array.from(fileMap.entries()).map(([name, content]) => ({ name, content }));

    // Processa cada arquivo individualmente com retry
    await Bluebird.mapSeries(files, async (file, fileIndex) => {
      const MAX_RETRIES = 3;
      const RETRY_DELAY = 2000; // 2 segundos
      let processedSuccessfully = false;

      console.log(`üìã Processando arquivo ${fileIndex + 1}/${files.length}: ${file.name}`);

      for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
        try {
          console.log(`üìÑ Tentativa ${attempt}/${MAX_RETRIES} para ${file.name}`);

          await brainstormFilesMulti({
            files: [file],
            providersModels: providerModelOptions,
            systemPrompt: SYSTEM_PROMPT,
            temperature: IA_TEMPERATURE,
          });

          console.log(`‚úÖ ${file.name} processado com sucesso`);
          processedSuccessfully = true;
          break; // Sucesso, sai do loop de retry
        } catch (err: unknown) {
          const errorMsg = err instanceof Error ? err.message : String(err);

          if (attempt === MAX_RETRIES) {
            // Registra falha final apenas ap√≥s esgotar todas as tentativas
            console.error(
              `‚ùå Falha final para ${file.name} ap√≥s ${MAX_RETRIES} tentativas: ${errorMsg}`
            );
            console.warn(`‚ö†Ô∏è Continuando com pr√≥ximo arquivo para manter ordem sequencial`);

            // Registra erro no log apenas na falha final
            appendBrainstormResult({
              provider: 'multiple',
              model: 'multiple',
              file: file.name,
              result: `Error ap√≥s ${MAX_RETRIES} tentativas: ${errorMsg}`,
              temperature: IA_TEMPERATURE,
              files: [file.name],
            });
          } else {
            console.warn(`‚ö†Ô∏è Tentativa ${attempt} falhou para ${file.name}: ${errorMsg}`);
            console.log(`üîÑ Aguardando ${RETRY_DELAY}ms antes da pr√≥xima tentativa...`);
            await new Promise((resolve) => setTimeout(resolve, RETRY_DELAY));
          }
        }
      }

      // Log de status do arquivo processado
      if (processedSuccessfully) {
        console.log(`‚úÖ Arquivo ${fileIndex + 1}/${files.length} conclu√≠do: ${file.name}`);
      } else {
        console.log(`‚ùå Arquivo ${fileIndex + 1}/${files.length} falhou: ${file.name}`);
      }

      // Delay entre arquivos para manter estabilidade
      await new Promise((resolve) => setTimeout(resolve, 500));
    });

    console.log('‚úÖ Multi-IA brainstorm finished! See results in', RESULT_FILE_PATH);
  } catch (err: unknown) {
    const errorMsg = err instanceof Error ? err.message : String(err);
    console.error('‚ùå Error running brainstorm:', errorMsg);
  }
};

//! ORIGINAL - N√ÉO MODIFICAR
// const run = async () => {
//   if (fs.existsSync(path.join('BRAINSTORM-RESULTS.md'))) {
//     fs.rmSync(path.join('BRAINSTORM-RESULTS.md'));
//   }

//   try {
//     console.log('üöÄ Starting multi-IA brainstorm for textgen-webui stack files...');
//     // Deduplica arquivos por nome
//     const fileMap = new Map<string, string>();
//     for (const filePath of textgenStackFileList) {
//       if (!fileMap.has(filePath)) {
//         fileMap.set(
//           filePath,
//           fs.existsSync(filePath) ? fs.readFileSync(filePath, 'utf8') : '[File not found]'
//         );
//       }
//     }
//     const files = Array.from(fileMap.entries()).map(([name, content]) => ({ name, content }));
//     console.log(
//       `[DEBUG] Arquivos para brainstorm:`,
//       files.map((f) => f.name)
//     );

//     await brainstormFilesMulti({
//       files,
//       providersModels: providerModelOptions,
//       systemPrompt: SYSTEM_PROMPT,
//       temperature: IA_TEMPERATURE,
//     });
//     console.log('‚úÖ Multi-IA brainstorm finished! See results in', RESULT_FILE_PATH);
//   } catch (err: unknown) {
//     const errorMsg = err instanceof Error ? err.message : String(err);
//     console.error('‚ùå Error running brainstorm:', errorMsg);
//   }
// };

run().catch(console.error);
