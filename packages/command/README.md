# Slice Command-R Server

Servidor Command-R compatÃ­vel com a API OpenAI, desenvolvido para o ecossistema Slice/ALIVE.

## ğŸš€ Quick Start

```bash
# Setup completo (instala dependÃªncias e baixa modelos)
task setup

# Iniciar em modo desenvolvimento
task dev

# Ou iniciar em modo produÃ§Ã£o
task start
```

O servidor estarÃ¡ disponÃ­vel em `http://localhost:5143`

## ğŸ“‹ Funcionalidades

- âœ… **OpenAI API Compatible**: Endpoints `/v1/chat/completions`, `/v1/models`
- âœ… **Command-R Integration**: Modelo Command-R nativo otimizado para CPU
- âœ… **CPU-Only Optimization**: Configurado para mÃ¡xima performance em CPU
- âœ… **Streaming Support**: Respostas em tempo real via Server-Sent Events
- âœ… **Health Monitoring**: Endpoints de saÃºde e monitoramento
- âœ… **Type Safety**: ValidaÃ§Ã£o completa com Pydantic
- âœ… **Modern Stack**: FastAPI + PDM + async/await

## ğŸš€ CPU-Only Performance

O Command-R foi especificamente otimizado para CPU, oferecendo:

- **Melhor Performance**: Command-R roda mais eficientemente em CPU que GPU
- **Threading Otimizado**: Configurado para aproveitar mÃºltiplos cores de CPU
- **Memory Efficient**: Otimizado para uso eficiente de RAM
- **No CUDA Required**: Funciona em qualquer mÃ¡quina sem necessidade de GPU

## ğŸ—ï¸ Arquitetura

```
server/
â”œâ”€â”€ __init__.py           # VersÃ£o e metadados
â”œâ”€â”€ main.py              # AplicaÃ§Ã£o FastAPI (mÃ­nima)
â”œâ”€â”€ config.py            # ConfiguraÃ§Ã£o e inicializaÃ§Ã£o
â”œâ”€â”€ constants.py         # Todas as constantes centralizadas
â”œâ”€â”€ api/                 # Endpoints organizados por recurso
â”‚   â”œâ”€â”€ chat.py         # /v1/chat/completions
â”‚   â”œâ”€â”€ completions.py  # /v1/completions
â”‚   â”œâ”€â”€ models.py       # /v1/models
â”‚   â””â”€â”€ health.py       # /health
â”œâ”€â”€ services/           # LÃ³gica de negÃ³cio e integraÃ§Ã£o
â”‚   â””â”€â”€ __init__.py    # CommandRService
â””â”€â”€ models/            # Schemas Pydantic
    â””â”€â”€ __init__.py    # Request/Response models
```

## ğŸ› ï¸ Comandos DisponÃ­veis

| Comando | DescriÃ§Ã£o |
|---------|-----------|
| `task setup` | Setup completo (install + check) |
| `task install` | Instalar dependÃªncias + baixar modelos |
| `task dev` | Servidor em modo desenvolvimento (reload) |
| `task start` | Servidor em modo produÃ§Ã£o |
| `task test` | Executar todos os testes |
| `task lint` | Linting e verificaÃ§Ã£o de tipos |
| `task format` | Formatar cÃ³digo automaticamente |
| `task clean` | Limpar arquivos temporÃ¡rios |
| `task check` | Verificar se tudo estÃ¡ funcionando |

## ğŸ“¡ API Endpoints

### Chat Completions
```bash
POST /v1/chat/completions
Content-Type: application/json

{
  "model": "command-r",
  "messages": [
    {"role": "user", "content": "Hello!"}
  ],
  "max_tokens": 100,
  "temperature": 0.7,
  "stream": false
}
```

### Streaming
```bash
POST /v1/chat/completions
Content-Type: application/json

{
  "model": "command-r",
  "messages": [{"role": "user", "content": "Hello!"}],
  "stream": true
}
```

### List Models
```bash
GET /v1/models
```

### Health Check
```bash
GET /health
```

## ğŸ“‘ ParÃ¢metros dos Endpoints

### /v1/chat/completions e /v1/completions

| ParÃ¢metro     | Tipo                | ObrigatÃ³rio | DescriÃ§Ã£o                                                                                 | Exemplo                         |
|---------------|---------------------|-------------|-------------------------------------------------------------------------------------------|---------------------------------|
| model         | string              | Sim         | Nome do modelo a ser usado (ex: command-r-small, command-r-medium, command-r-large)        | "command-r-small"              |
| messages      | array de objetos    | Sim (chat)  | Lista de mensagens (role: system/user/assistant, content: texto, name: opcional)           | [{"role": "user", "content": "Oi"}] |
| prompt        | string/array        | Sim (text)  | Prompt para completar (apenas em /completions)                                             | "Explique o conceito de IA"     |
| max_tokens    | int                 | NÃ£o         | MÃ¡ximo de tokens a gerar                                                                  | 100                             |
| temperature   | float (0.0â€“2.0)     | NÃ£o         | Aleatoriedade da resposta                                                                 | 0.7                             |
| top_p         | float (0.0â€“1.0)     | NÃ£o         | Amostragem nucleus/top-p                                                                 | 1.0                             |
| stream        | bool                | NÃ£o         | Se deve usar streaming (resposta em tempo real)                                           | false                           |
| stop          | string/array        | NÃ£o         | SequÃªncias de parada para interromper a geraÃ§Ã£o                                            | "\n" ou ["\n", "Fim"]            |

### /v1/embeddings

| ParÃ¢metro     | Tipo                | ObrigatÃ³rio | DescriÃ§Ã£o                                                                                 | Exemplo                         |
|---------------|---------------------|-------------|-------------------------------------------------------------------------------------------|---------------------------------|
| input         | string/array        | Sim         | Texto(s) para gerar embedding                                                             | "Texto para embed"              |
| model         | string              | Sim         | Nome do modelo a ser usado                                                                | "command-r-small"              |

> Consulte os exemplos de request acima para ver como montar cada payload.

## âš™ï¸ ConfiguraÃ§Ã£o

Todas as configuraÃ§Ãµes estÃ£o centralizadas em `server/constants.py`:

```python
# Server
SERVER_PORT = 5143
SERVER_HOST = "0.0.0.0"

# Model (CPU-optimized)
MODEL_NAME = "command-r"
MAX_TOKENS_DEFAULT = 4096
TEMPERATURE_DEFAULT = 0.7

# CPU optimization
TORCH_DEVICE = "cpu"
TORCH_THREADS = 8
USE_GPU = False
FORCE_CPU_ONLY = True

# Paths
MODELS_DIR = '/media/data/models'
CACHE_DIR = "./cache"
LOGS_DIR = "./logs"
```

### CPU-Only Configuration

O servidor estÃ¡ configurado para usar **apenas CPU**, seguindo o Checkpoint #019:

- `FORCE_CPU_ONLY = True`: ForÃ§a uso exclusivo de CPU
- `CUDA_VISIBLE_DEVICES = ""`: Desabilita acesso Ã  GPU
- `TORCH_THREADS = 8`: Otimiza threads para CPU
- `torch>=2.1.0+cpu`: VersÃ£o CPU-only do PyTorch

## ğŸ§ª Testes

O projeto segue uma estratÃ©gia rigorosa de testes baseada nos checkpoints:

```bash
# Executar todos os testes
task test

# Executar testes especÃ­ficos
pdm run pytest tests/test_constants.py -v
pdm run pytest tests/test_api_compliance.py -v
pdm run pytest tests/test_organization.py -v
```

### Cobertura de Testes

- âœ… **Estrutura**: ValidaÃ§Ã£o de diretÃ³rios e organizaÃ§Ã£o
- âœ… **Constantes**: VerificaÃ§Ã£o de valores configurÃ¡veis
- âœ… **API**: Compatibilidade com OpenAI API
- âœ… **OrganizaÃ§Ã£o**: SeparaÃ§Ã£o de responsabilidades
- âœ… **Taskfile**: ValidaÃ§Ã£o de tasks e comandos

## ğŸ”§ Desenvolvimento

### Requisitos
- Python 3.9+
- PDM (Package manager)
- Task (Task runner)

### Setup para Desenvolvimento
```bash
git clone <repo>
cd packages/command
task setup
```

### Estrutura de Commits
Seguimos o padrÃ£o Slice/ALIVE para commits:
- `feat:` Nova funcionalidade
- `fix:` CorreÃ§Ã£o de bug
- `test:` AdiÃ§Ã£o/modificaÃ§Ã£o de testes
- `docs:` DocumentaÃ§Ã£o
- `refactor:` RefatoraÃ§Ã£o sem mudanÃ§a de funcionalidade

### Code Style
- **Linting**: Ruff + Black + MyPy
- **Naming**: Nomes descritivos, evitar variÃ¡veis genÃ©ricas
- **Performance**: Priorizar cÃ³digo legÃ­vel e performÃ¡tico
- **Lines**: MÃ­nimo de linhas para resolver problemas

## ğŸš¦ Status do Projeto

### Checkpoints Implementados âœ…

1. **#001** - Estrutura de diretÃ³rios (`server/`, `tests/`)
2. **#002** - Taskfile completo com download de modelos
3. **#003** - OrganizaÃ§Ã£o modular (anti-pattern "pythonzeira")
4. **#006** - PDM como gerenciador de pacotes
5. **#008** - Porta definida em constants (nÃ£o .env)
6. **#009** - Constantes centralizadas (sem nÃºmeros mÃ¡gicos)
7. **#010** - Taskfile como interface principal
8. **#011** - Compatibilidade com OpenAI API
9. **#013** - Estrutura de pastas seguindo boas prÃ¡ticas

### Em Desenvolvimento ğŸ”„

- **Checkpoint #004/005** - FunÃ§Ãµes puras e robustez mÃ¡xima
- **Checkpoint #014** - Encapsulamento do modelo Command-R
- **Checkpoint #017** - Cache e load balancing

### Futuro ğŸ“‹

- IntegraÃ§Ã£o real com modelo Command-R
- Sistema de cache avanÃ§ado
- MÃ©tricas e observabilidade
- Deploy automatizado

## ğŸ¤ ContribuiÃ§Ã£o

1. Siga os checkpoints definidos em `PROJECT.md`
2. Execute `task check` antes de commits
3. Adicione testes para novas funcionalidades
4. Mantenha a documentaÃ§Ã£o atualizada

## ğŸ“š Links Ãšteis

- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [PDM Documentation](https://pdm.fming.dev/)
- [Task Documentation](https://taskfile.dev/)

## ğŸ§  Modelos DisponÃ­veis

Todos os modelos sÃ£o otimizados para CPU e seguem a convenÃ§Ã£o de nomes baseada no tamanho:

- `"command-r-small"`   â€” Modelo leve, ideal para respostas rÃ¡pidas e baixo consumo de memÃ³ria.
- `"command-r-medium"`  â€” EquilÃ­brio entre performance e qualidade.
- `"command-r-large"`   â€” Mais capacidade, respostas mais elaboradas e maior contexto.

> **Nota:** Consulte o endpoint `/v1/models` para ver quais variantes estÃ£o carregadas no momento.

### Exemplo de uso:
```json
{
  "model": "command-r-small",
  "messages": [
    {"role": "user", "content": "OlÃ¡, tudo bem?"}
  ],
  "max_tokens": 100
}
```
