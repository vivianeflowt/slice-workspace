"""
ü§ñ Model Downloader - Slice/ALIVE Providers Server
Download e gerenciamento de modelos HuggingFace.

Princ√≠pios CONCEPTS.md:
- Baixo Recurso: CPU-only, download incremental
- Incrementalismo: 1 modelo por vez, valida√ß√£o cont√≠nua
- Plug-and-Play: Auto-cria√ß√£o de diret√≥rios
- Offline-First: Cache local obrigat√≥rio
"""

import logging
import os
import shutil
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

import torch
from transformers import AutoConfig, AutoModel, AutoTokenizer
from server.constants import MODEL_CACHE_DIR, DEFAULT_MODELS, MAX_DOWNLOAD_RETRIES

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelDownloader:
    """
    Gerenciador de download e cache de modelos HuggingFace.

    Funcionalidades:
    - Download sequencial (1 modelo por vez)
    - Auto-cria√ß√£o de diret√≥rios
    - Valida√ß√£o de modelos baixados
    - Limpeza de cache
    """

    def __init__(
        self, models_dir: Optional[str] = None, cache_dir: Optional[str] = None
    ):
        """
        Inicializa downloader com auto-cria√ß√£o de paths.

        Args:
            models_dir: Diret√≥rio para modelos (usa constants se None)
            cache_dir: Diret√≥rio para cache (usa constants se None)
        """
        # Importar constants aqui para evitar circular imports
        try:
            from server.constants import (
                CACHE_DIR,
                DEFAULT_MODELS,
                FORCE_CPU_ONLY,
                MODELS_DIR,
            )

            self.models_dir = Path(models_dir or MODELS_DIR)
            self.cache_dir = Path(cache_dir or CACHE_DIR)
            self.force_cpu_only = FORCE_CPU_ONLY
            self.default_models = DEFAULT_MODELS
        except ImportError:
            # Fallback se constants n√£o dispon√≠vel
            self.models_dir = Path(models_dir or "/media/data/llvm/general/models")
            self.cache_dir = Path(cache_dir or "/media/data/llvm/general/cache")
            self.force_cpu_only = True
            self.default_models = {}

        # Auto-criar diret√≥rios necess√°rios
        self._ensure_directories()

        # Configurar torch para CPU-only se necess√°rio
        if self.force_cpu_only:
            torch.set_default_tensor_type("torch.FloatTensor")
            os.environ["CUDA_VISIBLE_DEVICES"] = ""

    def _ensure_directories(self):
        """
        Garante que todos os diret√≥rios necess√°rios existem.
        """
        directories = [
            self.models_dir,
            self.cache_dir,
            self.models_dir / "classify",
            self.models_dir / "embed",
            self.models_dir / "pos_tag",
            self.cache_dir / "huggingface",
            self.cache_dir / "transformers",
        ]

        for directory in directories:
            try:
                directory.mkdir(parents=True, exist_ok=True)
                logger.debug(f"üìÅ Diret√≥rio garantido: {directory}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Falha ao criar diret√≥rio {directory}: {e}")

    def download_model(self, model_name: str, max_retries: int = 3) -> bool:
        """
        Baixa um modelo espec√≠fico do HuggingFace (1 por vez).

        Args:
            model_name: Nome do modelo no HuggingFace Hub
            max_retries: N√∫mero m√°ximo de tentativas

        Returns:
            bool: True se download foi bem-sucedido
        """
        logger.info(f"üì• Iniciando download: {model_name}")

        # Verificar se j√° est√° baixado
        if self.is_model_downloaded(model_name):
            logger.info(f"‚úÖ Modelo j√° baixado: {model_name}")
            return True

        # Garantir diret√≥rios antes do download
        self._ensure_directories()

        for attempt in range(1, max_retries + 1):
            logger.info(f"üîÑ Tentativa {attempt}/{max_retries} para {model_name}")

            try:
                start_time = time.time()

                # Definir cache_dir para este modelo
                model_cache_path = (
                    self.cache_dir / "transformers" / model_name.replace("/", "--")
                )

                # Download sequencial dos componentes
                logger.info(f"üì¶ Baixando tokenizer...")
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    cache_dir=str(self.cache_dir),
                    force_download=False,
                    resume_download=True,
                )

                logger.info(f"üì¶ Baixando config...")
                config = AutoConfig.from_pretrained(
                    model_name,
                    cache_dir=str(self.cache_dir),
                    force_download=False,
                    resume_download=True,
                )

                logger.info(f"üì¶ Baixando modelo principal...")
                model = AutoModel.from_pretrained(
                    model_name,
                    cache_dir=str(self.cache_dir),
                    torch_dtype=torch.float32,  # CPU-optimized
                    device_map="cpu" if self.force_cpu_only else "auto",
                    force_download=False,
                    resume_download=True,
                )

                elapsed_time = time.time() - start_time

                # Validar download
                if self._validate_model(model_name, tokenizer, model, config):
                    logger.info(
                        f"‚úÖ Download conclu√≠do: {model_name} ({elapsed_time:.1f}s)"
                    )
                    return True
                else:
                    logger.warning(f"‚ö†Ô∏è Valida√ß√£o falhou para {model_name}")

            except Exception as e:
                logger.error(f"‚ùå Erro no download (tentativa {attempt}): {e}")

                if attempt < max_retries:
                    wait_time = 2**attempt  # Backoff exponencial
                    logger.info(
                        f"‚è≥ Aguardando {wait_time}s antes da pr√≥xima tentativa..."
                    )
                    time.sleep(wait_time)

        logger.error(
            f"üí• Falha no download ap√≥s {max_retries} tentativas: {model_name}"
        )
        return False

    def is_model_downloaded(self, model_name: str) -> bool:
        """
        Verifica se um modelo j√° foi baixado e est√° no cache.

        Args:
            model_name: Nome do modelo no HuggingFace Hub

        Returns:
            bool: True se o modelo j√° foi baixado, False caso contr√°rio
        """
        return (
            self.cache_dir / "transformers" / (model_name.replace("/", "--") + ".bin")
        ).exists()

    def _validate_model(
        self,
        model_name: str,
        tokenizer: AutoTokenizer,
        model: AutoModel,
        config: AutoConfig,
    ) -> bool:
        """
        Valida se um modelo baixado √© v√°lido.

        Args:
            model_name: Nome do modelo no HuggingFace Hub
            tokenizer: Tokenizer do modelo
            model: Modelo baixado
            config: Configura√ß√£o do modelo

        Returns:
            bool: True se o modelo √© v√°lido, False caso contr√°rio
        """
        try:
            # Testa tokeniza√ß√£o simples
            test_text = "Teste de valida√ß√£o"
            tokens = tokenizer.encode(test_text, return_tensors="pt")

            if len(tokens[0]) > 0:
                return True

        except Exception as e:
            logger.error(f"‚ùå Erro na valida√ß√£o do modelo {model_name}: {str(e)}")

        return False


def download_default_models() -> Dict[str, bool]:
    """
    Baixa todos os modelos padr√£o definidos em constants.py.

    Returns:
        Dicion√°rio com status de download de cada modelo
    """
    from server.utils.model_downloader import ModelDownloader
    model_downloader = ModelDownloader()
    print("üöÄ Iniciando download dos modelos padr√£o...")
    # Garantir que o diret√≥rio de cache existe (padr√£o Slice/ALIVE)
    Path(MODEL_CACHE_DIR).mkdir(parents=True, exist_ok=True)

    results = {}
    total_models = len(DEFAULT_MODELS)

    for i, (function_name, model_name) in enumerate(DEFAULT_MODELS.items(), 1):
        print(f"\n[{i}/{total_models}] Fun√ß√£o: {function_name}")

        # Tenta download com retry
        success = False
        for attempt in range(MAX_DOWNLOAD_RETRIES):
            if attempt > 0:
                print(f"  üîÑ Tentativa {attempt + 1}/{MAX_DOWNLOAD_RETRIES}")
                time.sleep(2)  # Pausa entre tentativas

            success = model_downloader.download_model(model_name)
            if success:
                break

        results[function_name] = success

        if not success:
            print(f"  ‚ö†Ô∏è  Falha ap√≥s {MAX_DOWNLOAD_RETRIES} tentativas")

    # Relat√≥rio final
    successful = sum(1 for v in results.values() if v)
    print(f"\nüìä Relat√≥rio final:")
    print(f"  ‚úÖ Sucessos: {successful}/{total_models}")
    print(f"  ‚ùå Falhas: {total_models - successful}/{total_models}")

    if successful == total_models:
        print("üéâ Todos os modelos foram baixados com sucesso!")
    else:
        print("‚ö†Ô∏è  Alguns modelos falharam. Verifique logs acima.")

    return results


def list_downloaded_models() -> List[Dict[str, Any]]:
    """
    Lista todos os modelos baixados no cache.

    Returns:
        Lista de dicion√°rios com informa√ß√µes dos modelos
    """
    cache_path = Path(MODEL_CACHE_DIR)
    if not cache_path.exists():
        print("üìÅ Cache de modelos n√£o encontrado.")
        return []

    models = []

    # Busca por diret√≥rios de modelos
    for model_dir in cache_path.rglob("*"):
        if model_dir.is_dir() and (model_dir / "config.json").exists():
            # Calcula tamanho do modelo
            size_bytes = sum(
                f.stat().st_size for f in model_dir.rglob("*") if f.is_file()
            )
            size_mb = size_bytes / (1024 * 1024)

            # Extrai nome do modelo do caminho
            model_name = str(model_dir.relative_to(cache_path))
            if "--" in model_name:
                model_name = model_name.replace("--", "/")

            models.append(
                {
                    "name": model_name,
                    "path": str(model_dir),
                    "size_mb": round(size_mb, 2),
                    "files": len(list(model_dir.rglob("*"))),
                }
            )

    if models:
        print(f"üì¶ Modelos baixados ({len(models)} encontrados):")
        for model in models:
            print(f"  ‚Ä¢ {model['name']} ({model['size_mb']} MB)")
    else:
        print("üì¶ Nenhum modelo encontrado no cache.")

    return models


def validate_model(model_name: str) -> bool:
    """
    Valida se um modelo est√° corretamente baixado e pode ser carregado.

    Args:
        model_name: Nome do modelo para validar

    Returns:
        True se modelo √© v√°lido, False caso contr√°rio
    """
    try:
        # Tenta carregar tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=MODEL_CACHE_DIR,
            local_files_only=True,
        )

        # Tenta carregar configura√ß√£o
        config = AutoConfig.from_pretrained(
            model_name,
            cache_dir=MODEL_CACHE_DIR,
            local_files_only=True,
        )

        # Testa tokeniza√ß√£o simples
        test_text = "Teste de valida√ß√£o"
        tokens = tokenizer.encode(test_text, return_tensors="pt")

        if len(tokens[0]) > 0:
            return True

    except Exception as e:
        print(f"‚ùå Erro na valida√ß√£o do modelo {model_name}: {str(e)}")

    return False


def cleanup_cache() -> Dict[str, Any]:
    """
    Limpa arquivos antigos e corrompidos do cache.

    Returns:
        Estat√≠sticas da limpeza
    """
    cache_path = Path(MODEL_CACHE_DIR)
    if not cache_path.exists():
        return {"cleaned": 0, "freed_mb": 0, "errors": []}

    cleaned_files = 0
    freed_bytes = 0
    errors = []

    print("üßπ Limpando cache de modelos...")

    # Remove arquivos tempor√°rios
    for temp_file in cache_path.rglob("*.tmp"):
        try:
            size = temp_file.stat().st_size
            temp_file.unlink()
            cleaned_files += 1
            freed_bytes += size
            print(f"  üóëÔ∏è  Removido: {temp_file.name}")
        except Exception as e:
            errors.append(f"Erro ao remover {temp_file}: {str(e)}")

    # Remove diret√≥rios vazios
    for empty_dir in cache_path.rglob("*"):
        if empty_dir.is_dir() and not any(empty_dir.iterdir()):
            try:
                empty_dir.rmdir()
                print(f"  üìÅ Diret√≥rio vazio removido: {empty_dir.name}")
            except Exception as e:
                errors.append(f"Erro ao remover diret√≥rio {empty_dir}: {str(e)}")

    freed_mb = freed_bytes / (1024 * 1024)

    print(f"‚ú® Limpeza conclu√≠da:")
    print(f"  ‚Ä¢ Arquivos removidos: {cleaned_files}")
    print(f"  ‚Ä¢ Espa√ßo liberado: {freed_mb:.2f} MB")
    if errors:
        print(f"  ‚Ä¢ Erros: {len(errors)}")

    return {
        "cleaned": cleaned_files,
        "freed_mb": round(freed_mb, 2),
        "errors": errors,
    }


def get_model_info(model_name: str) -> Dict[str, Any]:
    """
    Obt√©m informa√ß√µes detalhadas sobre um modelo.

    Args:
        model_name: Nome do modelo

    Returns:
        Dicion√°rio com informa√ß√µes do modelo
    """
    try:
        config = AutoConfig.from_pretrained(
            model_name,
            cache_dir=MODEL_CACHE_DIR,
            local_files_only=True,
        )

        return {
            "name": model_name,
            "architecture": (
                config.architectures[0] if config.architectures else "unknown"
            ),
            "vocab_size": getattr(config, "vocab_size", "unknown"),
            "hidden_size": getattr(config, "hidden_size", "unknown"),
            "num_layers": getattr(config, "num_hidden_layers", "unknown"),
            "max_position": getattr(config, "max_position_embeddings", "unknown"),
            "model_type": getattr(config, "model_type", "unknown"),
        }

    except Exception as e:
        return {
            "name": model_name,
            "error": str(e),
            "available": False,
        }


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        command = sys.argv[1]

        if command == "download":
            download_default_models()
        elif command == "list":
            list_downloaded_models()
        elif command == "cleanup":
            cleanup_cache()
        elif command == "validate":
            if len(sys.argv) > 2:
                model_name = sys.argv[2]
                is_valid = validate_model(model_name)
                print(
                    f"Modelo {model_name}: {'‚úÖ V√°lido' if is_valid else '‚ùå Inv√°lido'}"
                )
            else:
                print("Usage: python model_downloader.py validate <model_name>")
        else:
            print("Comandos dispon√≠veis: download, list, cleanup, validate")
    else:
        print("Usage: python model_downloader.py <command>")
        print("Comandos: download, list, cleanup, validate")
