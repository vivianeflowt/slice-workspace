"""
Utilit√°rio para download e gerenciamento de modelos HuggingFace.

Este m√≥dulo √© respons√°vel por:
- Baixar modelos padr√£o na instala√ß√£o
- Listar modelos dispon√≠veis 
- Validar modelos baixados
- Gerenciar cache de modelos
"""

import os
import time
from typing import List, Dict, Any
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModel, AutoConfig
from server.constants import (
    DEFAULT_MODELS,
    MODEL_CACHE_DIR,
    DOWNLOAD_TIMEOUT,
    MAX_DOWNLOAD_RETRIES,
    FORCE_CPU_ONLY,
)


def ensure_cache_dir() -> Path:
    """Garante que o diret√≥rio de cache existe."""
    cache_path = Path(MODEL_CACHE_DIR)
    cache_path.mkdir(parents=True, exist_ok=True)
    return cache_path


def download_model(model_name: str, force_cpu: bool = FORCE_CPU_ONLY) -> bool:
    """
    Baixa um modelo espec√≠fico do HuggingFace.
    
    Args:
        model_name: Nome do modelo no HuggingFace Hub
        force_cpu: Se deve for√ßar CPU apenas
        
    Returns:
        True se download foi bem-sucedido, False caso contr√°rio
    """
    print(f"üì• Baixando modelo: {model_name}")
    
    try:
        # Configura device para CPU apenas se necess√°rio
        if force_cpu:
            torch.set_default_tensor_type('torch.FloatTensor')
        
        # Baixa tokenizer
        print(f"  ‚îú‚îÄ‚îÄ Baixando tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=MODEL_CACHE_DIR,
            trust_remote_code=False,
        )
        
        # Baixa configura√ß√£o
        print(f"  ‚îú‚îÄ‚îÄ Baixando configura√ß√£o...")
        config = AutoConfig.from_pretrained(
            model_name,
            cache_dir=MODEL_CACHE_DIR,
            trust_remote_code=False,
        )
        
        # Baixa modelo
        print(f"  ‚îú‚îÄ‚îÄ Baixando modelo...")
        model = AutoModel.from_pretrained(
            model_name,
            cache_dir=MODEL_CACHE_DIR,
            torch_dtype=torch.float32 if force_cpu else "auto",
            device_map=None if force_cpu else "auto",
            trust_remote_code=False,
        )
        
        # Move para CPU se necess√°rio
        if force_cpu:
            model = model.to("cpu")
        
        print(f"  ‚îî‚îÄ‚îÄ ‚úÖ Modelo {model_name} baixado com sucesso!")
        return True
        
    except Exception as e:
        print(f"  ‚îî‚îÄ‚îÄ ‚ùå Erro ao baixar {model_name}: {str(e)}")
        return False


def download_default_models() -> Dict[str, bool]:
    """
    Baixa todos os modelos padr√£o definidos em constants.py.
    
    Returns:
        Dicion√°rio com status de download de cada modelo
    """
    print("üöÄ Iniciando download dos modelos padr√£o...")
    ensure_cache_dir()
    
    results = {}
    total_models = len(DEFAULT_MODELS)
    
    for i, (function_name, model_name) in enumerate(DEFAULT_MODELS.items(), 1):
        print(f"\n[{i}/{total_models}] Fun√ß√£o: {function_name}")
        
        # Tenta download com retry
        success = False
        for attempt in range(MAX_DOWNLOAD_RETRIES):
            if attempt > 0:
                print(f"  üîÑ Tentativa {attempt + 1}/{MAX_DOWNLOAD_RETRIES}")
                time.sleep(2)  # Pausa entre tentativas
            
            success = download_model(model_name)
            if success:
                break
        
        results[function_name] = success
        
        if not success:
            print(f"  ‚ö†Ô∏è  Falha ap√≥s {MAX_DOWNLOAD_RETRIES} tentativas")
    
    # Relat√≥rio final
    successful = sum(1 for v in results.values() if v)
    print(f"\nüìä Relat√≥rio final:")
    print(f"  ‚úÖ Sucessos: {successful}/{total_models}")
    print(f"  ‚ùå Falhas: {total_models - successful}/{total_models}")
    
    if successful == total_models:
        print("üéâ Todos os modelos foram baixados com sucesso!")
    else:
        print("‚ö†Ô∏è  Alguns modelos falharam. Verifique logs acima.")
    
    return results


def list_downloaded_models() -> List[Dict[str, Any]]:
    """
    Lista todos os modelos baixados no cache.
    
    Returns:
        Lista de dicion√°rios com informa√ß√µes dos modelos
    """
    cache_path = Path(MODEL_CACHE_DIR)
    if not cache_path.exists():
        print("üìÅ Cache de modelos n√£o encontrado.")
        return []
    
    models = []
    
    # Busca por diret√≥rios de modelos
    for model_dir in cache_path.rglob("*"):
        if model_dir.is_dir() and (model_dir / "config.json").exists():
            # Calcula tamanho do modelo
            size_bytes = sum(
                f.stat().st_size 
                for f in model_dir.rglob("*") 
                if f.is_file()
            )
            size_mb = size_bytes / (1024 * 1024)
            
            # Extrai nome do modelo do caminho
            model_name = str(model_dir.relative_to(cache_path))
            if "--" in model_name:
                model_name = model_name.replace("--", "/")
            
            models.append({
                "name": model_name,
                "path": str(model_dir),
                "size_mb": round(size_mb, 2),
                "files": len(list(model_dir.rglob("*"))),
            })
    
    if models:
        print(f"üì¶ Modelos baixados ({len(models)} encontrados):")
        for model in models:
            print(f"  ‚Ä¢ {model['name']} ({model['size_mb']} MB)")
    else:
        print("üì¶ Nenhum modelo encontrado no cache.")
    
    return models


def validate_model(model_name: str) -> bool:
    """
    Valida se um modelo est√° corretamente baixado e pode ser carregado.
    
    Args:
        model_name: Nome do modelo para validar
        
    Returns:
        True se modelo √© v√°lido, False caso contr√°rio
    """
    try:
        # Tenta carregar tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=MODEL_CACHE_DIR,
            local_files_only=True,
        )
        
        # Tenta carregar configura√ß√£o
        config = AutoConfig.from_pretrained(
            model_name,
            cache_dir=MODEL_CACHE_DIR,
            local_files_only=True,
        )
        
        # Testa tokeniza√ß√£o simples
        test_text = "Teste de valida√ß√£o"
        tokens = tokenizer.encode(test_text, return_tensors="pt")
        
        if len(tokens[0]) > 0:
            return True
        
    except Exception as e:
        print(f"‚ùå Erro na valida√ß√£o do modelo {model_name}: {str(e)}")
    
    return False


def cleanup_cache() -> Dict[str, Any]:
    """
    Limpa arquivos antigos e corrompidos do cache.
    
    Returns:
        Estat√≠sticas da limpeza
    """
    cache_path = Path(MODEL_CACHE_DIR)
    if not cache_path.exists():
        return {"cleaned": 0, "freed_mb": 0, "errors": []}
    
    cleaned_files = 0
    freed_bytes = 0
    errors = []
    
    print("üßπ Limpando cache de modelos...")
    
    # Remove arquivos tempor√°rios
    for temp_file in cache_path.rglob("*.tmp"):
        try:
            size = temp_file.stat().st_size
            temp_file.unlink()
            cleaned_files += 1
            freed_bytes += size
            print(f"  üóëÔ∏è  Removido: {temp_file.name}")
        except Exception as e:
            errors.append(f"Erro ao remover {temp_file}: {str(e)}")
    
    # Remove diret√≥rios vazios
    for empty_dir in cache_path.rglob("*"):
        if empty_dir.is_dir() and not any(empty_dir.iterdir()):
            try:
                empty_dir.rmdir()
                print(f"  üìÅ Diret√≥rio vazio removido: {empty_dir.name}")
            except Exception as e:
                errors.append(f"Erro ao remover diret√≥rio {empty_dir}: {str(e)}")
    
    freed_mb = freed_bytes / (1024 * 1024)
    
    print(f"‚ú® Limpeza conclu√≠da:")
    print(f"  ‚Ä¢ Arquivos removidos: {cleaned_files}")
    print(f"  ‚Ä¢ Espa√ßo liberado: {freed_mb:.2f} MB")
    if errors:
        print(f"  ‚Ä¢ Erros: {len(errors)}")
    
    return {
        "cleaned": cleaned_files,
        "freed_mb": round(freed_mb, 2),
        "errors": errors,
    }


def get_model_info(model_name: str) -> Dict[str, Any]:
    """
    Obt√©m informa√ß√µes detalhadas sobre um modelo.
    
    Args:
        model_name: Nome do modelo
        
    Returns:
        Dicion√°rio com informa√ß√µes do modelo
    """
    try:
        config = AutoConfig.from_pretrained(
            model_name,
            cache_dir=MODEL_CACHE_DIR,
            local_files_only=True,
        )
        
        return {
            "name": model_name,
            "architecture": config.architectures[0] if config.architectures else "unknown",
            "vocab_size": getattr(config, "vocab_size", "unknown"),
            "hidden_size": getattr(config, "hidden_size", "unknown"),
            "num_layers": getattr(config, "num_hidden_layers", "unknown"),
            "max_position": getattr(config, "max_position_embeddings", "unknown"),
            "model_type": getattr(config, "model_type", "unknown"),
        }
        
    except Exception as e:
        return {
            "name": model_name,
            "error": str(e),
            "available": False,
        }


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "download":
            download_default_models()
        elif command == "list":
            list_downloaded_models()
        elif command == "cleanup":
            cleanup_cache()
        elif command == "validate":
            if len(sys.argv) > 2:
                model_name = sys.argv[2]
                is_valid = validate_model(model_name)
                print(f"Modelo {model_name}: {'‚úÖ V√°lido' if is_valid else '‚ùå Inv√°lido'}")
            else:
                print("Usage: python model_downloader.py validate <model_name>")
        else:
            print("Comandos dispon√≠veis: download, list, cleanup, validate")
    else:
        print("Usage: python model_downloader.py <command>")
        print("Comandos: download, list, cleanup, validate")
