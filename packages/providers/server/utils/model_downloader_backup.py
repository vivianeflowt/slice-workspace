"""
ü§ñ Model Downloader - Slice/ALIVE Providers Server
Download e gerenciamento de modelos HuggingFace.

Princ√≠pios CONCEPTS.md:
- Baixo Recurso: CPU-only, download incremental
- Incrementalismo: 1 modelo por vez, valida√ß√£o cont√≠nua
- Plug-and-Play: Auto-cria√ß√£o de diret√≥rios
- Offline-First: Cache local obrigat√≥rio
"""

import logging
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""
import shutil
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

import torch
import psutil  # Adicionado para controle de prioridade
from transformers import AutoConfig, AutoModel, AutoTokenizer
from huggingface_hub import snapshot_download  # Para download sem carregamento
from server.constants import MODEL_CACHE_DIR, DEFAULT_MODELS, MAX_DOWNLOAD_RETRIES

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def set_low_priority():
    """
    Define a prioridade do processo atual para baixa (CPU e I/O).
    Isso evita que o download congele a m√°quina.
    """
    try:
        process = psutil.Process(os.getpid())
        if os.name == 'nt':  # Windows
            process.nice(psutil.BELOW_NORMAL_PRIORITY_CLASS)
        else:  # Linux/macOS
            process.nice(19)  # A prioridade mais baixa
            try:
                # Tenta definir I/O priority para idle (mais suave para o disco)
                process.ionice(psutil.IOPRIO_CLASS_IDLE)
                logger.info("‚úÖ Prioridade de I/O definida como IDLE.")
            except (AttributeError, OSError):
                # ionice pode n√£o estar dispon√≠vel ou precisar de permiss√µes
                logger.info("‚ö†Ô∏è N√£o foi poss√≠vel definir prioridade de I/O.")

        logger.info("‚úÖ Prioridade do processo de download definida como baixa.")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel definir a prioridade do processo: {e}")


def configure_minimal_resources():
    """
    Configura PyTorch e HuggingFace para usar recursos m√≠nimos.
    """
    # Limita PyTorch a usar apenas 1 thread (evita sobrecarregar CPU)
    torch.set_num_threads(1)
    torch.set_num_interop_threads(1)

    # For√ßa CPU e desabilita qualquer acelera√ß√£o
    os.environ["CUDA_VISIBLE_DEVICES"] = ""
    os.environ["MKL_NUM_THREADS"] = "1"
    os.environ["OPENBLAS_NUM_THREADS"] = "1"
    os.environ["OMP_NUM_THREADS"] = "1"

    # Configura HuggingFace para download mais gentil
    os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "300"  # Timeout maior
    os.environ["HF_HUB_OFFLINE"] = "0"  # For√ßa online mode

    logger.info("‚úÖ Configura√ß√£o de recursos m√≠nimos aplicada.")


class ModelDownloader:
    """
    Gerenciador de download e cache de modelos HuggingFace.

    Funcionalidades:
    - Download sequencial (1 modelo por vez)
    - Auto-cria√ß√£o de diret√≥rios
    - Valida√ß√£o de modelos baixados
    - Limpeza de cache
    """

    def __init__(
        self, models_dir: Optional[str] = None, cache_dir: Optional[str] = None
    ):
        """
        Inicializa downloader com auto-cria√ß√£o de paths.

        Args:
            models_dir: Diret√≥rio para modelos (usa constants se None)
            cache_dir: Diret√≥rio para cache (usa constants se None)
        """
        # Configurar recursos m√≠nimos ANTES de qualquer opera√ß√£o
        configure_minimal_resources()

        # Importar constants aqui para evitar circular imports
        try:
            from server.constants import (
                CACHE_DIR,
                DEFAULT_MODELS,
                FORCE_CPU_ONLY,
                MODELS_DIR,
            )

            self.models_dir = Path(models_dir or MODELS_DIR)
            self.cache_dir = Path(cache_dir or CACHE_DIR)
            self.force_cpu_only = FORCE_CPU_ONLY
            self.default_models = DEFAULT_MODELS
        except ImportError:
            # Fallback se constants n√£o dispon√≠vel
            self.models_dir = Path(models_dir or "/media/data/llvm/general/models")
            self.cache_dir = Path(cache_dir or "/media/data/llvm/general/cache")
            self.force_cpu_only = True
            self.default_models = {}

        # Auto-criar diret√≥rios necess√°rios
        self._ensure_directories()

        # Configurar torch para CPU-only se necess√°rio
        if self.force_cpu_only:
            torch.set_default_tensor_type("torch.FloatTensor")
            os.environ["CUDA_VISIBLE_DEVICES"] = ""

    def _ensure_directories(self):
        """
        Garante que todos os diret√≥rios necess√°rios existem.
        """
        # O cache da HuggingFace √© gerenciado pela pr√≥pria lib,
        # precisamos apenas garantir o diret√≥rio raiz.
        directories = [
            self.models_dir,
            self.cache_dir,
        ]

        for directory in directories:
            try:
                directory.mkdir(parents=True, exist_ok=True)
                logger.debug(f"üìÅ Diret√≥rio garantido: {directory}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Falha ao criar diret√≥rio {directory}: {e}")

    def download_model(self, model_name: str, max_retries: int = 3) -> bool:
        """
        Baixa um modelo espec√≠fico do HuggingFace SEM carregar na mem√≥ria.

        Esta vers√£o otimizada faz APENAS o download dos arquivos,
        sem carregar o modelo na RAM, evitando travamentos.

        Args:
            model_name: Nome do modelo no HuggingFace Hub
            max_retries: N√∫mero m√°ximo de tentativas

        Returns:
            bool: True se download foi bem-sucedido
        """
        logger.info(f"üì• Iniciando download: {model_name}")

        # Tenta carregar localmente primeiro (verifica√ß√£o mais robusta)
        if self.is_model_downloaded(model_name):
            logger.info(f"‚úÖ Modelo j√° baixado e validado localmente: {model_name}")
            return True

        # Garantir diret√≥rios antes do download
        self._ensure_directories()

        for attempt in range(1, max_retries + 1):
            logger.info(f"üîÑ Tentativa {attempt}/{max_retries} para {model_name}")

            try:
                start_time = time.time()

                # OTIMIZA√á√ÉO CR√çTICA: Usar snapshot_download em vez de from_pretrained
                # Isso baixa APENAS os arquivos, sem carregar na mem√≥ria
                logger.info(f"üì¶ Baixando arquivos do modelo (sem carregar na RAM)...")

                model_path = snapshot_download(
                    repo_id=model_name,
                    cache_dir=str(self.cache_dir),
                    resume_download=True,
                    max_workers=1,  # Limite de 1 thread para download mais suave
                    local_files_only=False,
                )

                elapsed_time = time.time() - start_time

                # Valida√ß√£o leve: apenas verificar se os arquivos essenciais existem
                if self._validate_downloaded_files(model_name, model_path):
                    logger.info(
                        f"‚úÖ Download conclu√≠do: {model_name} ({elapsed_time:.1f}s)"
                    )
                    return True
                else:
                    logger.warning(f"‚ö†Ô∏è Valida√ß√£o de arquivos falhou para {model_name}")

            except Exception as e:
                logger.error(f"‚ùå Erro no download (tentativa {attempt}): {e}")

                if attempt < max_retries:
                    wait_time = 2**attempt  # Backoff exponencial
                    logger.info(
                        f"‚è≥ Aguardando {wait_time}s antes da pr√≥xima tentativa..."
                    )
                    time.sleep(wait_time)

        logger.error(
            f"üí• Falha no download ap√≥s {max_retries} tentativas: {model_name}"
        )
        return False

    def is_model_downloaded(self, model_name: str) -> bool:
        """
        Verifica se um modelo j√° foi baixado e pode ser carregado localmente.
        Esta √© uma verifica√ß√£o mais robusta do que procurar por um arquivo .bin.

        Args:
            model_name: Nome do modelo no HuggingFace Hub

        Returns:
            bool: True se o modelo j√° foi baixado, False caso contr√°rio
        """
        try:
            # Tenta carregar a configura√ß√£o SOMENTE de arquivos locais.
            # Se funcionar, o modelo est√° presente e provavelmente √≠ntegro.
            _ = AutoConfig.from_pretrained(
                model_name, cache_dir=str(self.cache_dir), local_files_only=True
            )
            return True
        except Exception:
            return False

    def _validate_downloaded_files(self, model_name: str, model_path: str) -> bool:
        """
        Valida√ß√£o LEVE que verifica apenas se os arquivos essenciais existem.
        N√£o carrega o modelo na mem√≥ria, evitando travamentos.

        Args:
            model_name: Nome do modelo no HuggingFace Hub
            model_path: Caminho onde o modelo foi baixado

        Returns:
            bool: True se os arquivos essenciais existem
        """
        try:
            model_dir = Path(model_path)

            # Verifica arquivos essenciais
            essential_files = [
                "config.json",  # Configura√ß√£o do modelo
            ]

            # Verifica se pelo menos os arquivos de configura√ß√£o existem
            for file_name in essential_files:
                file_path = model_dir / file_name
                if not file_path.exists():
                    logger.error(f"‚ùå Arquivo essencial n√£o encontrado: {file_path}")
                    return False

            # Verifica se existe pelo menos um arquivo de modelo (.bin, .safetensors, .onnx, etc.)
            model_files = list(model_dir.glob("*.bin")) + \
                         list(model_dir.glob("*.safetensors")) + \
                         list(model_dir.glob("*.onnx"))

            if not model_files:
                logger.error(f"‚ùå Nenhum arquivo de modelo encontrado em: {model_path}")
                return False

            # Se chegou at√© aqui, o modelo parece estar √≠ntegro
            logger.info(f"üîç Valida√ß√£o de arquivos bem-sucedida para {model_name}")
            logger.info(f"   üìÅ Localiza√ß√£o: {model_path}")
            logger.info(f"   üìÑ Arquivos de modelo: {len(model_files)}")

            return True

        except Exception as e:
            logger.error(f"‚ùå Erro na valida√ß√£o de arquivos para {model_name}: {str(e)}")
            return False

    def _validate_model(
        self,
        model_name: str,
        tokenizer: AutoTokenizer,
        model: AutoModel,
    ) -> bool:
        """
        Valida se um modelo baixado √© v√°lido.

        Args:
            model_name: Nome do modelo no HuggingFace Hub
            tokenizer: Tokenizer do modelo
            model: Modelo baixado

        Returns:
            bool: True se o modelo √© v√°lido, False caso contr√°rio
        """
        try:
            # Testa tokeniza√ß√£o e uma pequena infer√™ncia para garantir
            test_text = "Teste de valida√ß√£o do modelo."
            inputs = tokenizer(test_text, return_tensors="pt")

            # Move os inputs para a CPU se necess√°rio
            if self.force_cpu_only:
                inputs = {k: v.to("cpu") for k, v in inputs.items()}

            with torch.no_grad():
                outputs = model(**inputs)

            # Verifica se a sa√≠da tem a estrutura esperada
            if hasattr(outputs, 'last_hidden_state') and outputs.last_hidden_state.shape[1] > 0:
                logger.info(f"üîç Valida√ß√£o bem-sucedida para {model_name}")
                return True

        except Exception as e:
            logger.error(f"‚ùå Erro na valida√ß√£o do modelo {model_name}: {str(e)}")

        return False


def download_default_models() -> Dict[str, bool]:
    """
    Baixa todos os modelos padr√£o definidos em constants.py.

    Returns:
        Dicion√°rio com status de download de cada modelo
    """
    from server.utils.model_downloader import ModelDownloader

    # Configura√ß√µes para evitar travamentos
    configure_minimal_resources()
    set_low_priority()

    model_downloader = ModelDownloader()
    print("üöÄ Iniciando download dos modelos padr√£o...")
    print("‚öôÔ∏è  Modo de baixo recurso ativado para evitar travamentos.")

    # Garantir que o diret√≥rio de cache existe (padr√£o Slice/ALIVE)
    Path(MODEL_CACHE_DIR).mkdir(parents=True, exist_ok=True)

    results = {}
    total_models = len(DEFAULT_MODELS)

    for i, (function_name, model_name) in enumerate(DEFAULT_MODELS.items(), 1):
        print(f"\n[{i}/{total_models}] Fun√ß√£o: {function_name}")

        # Tenta download com retry
        success = False
        for attempt in range(MAX_DOWNLOAD_RETRIES):
            if attempt > 0:
                print(f"  üîÑ Tentativa {attempt + 1}/{MAX_DOWNLOAD_RETRIES}")
                time.sleep(2)  # Pausa entre tentativas

            success = model_downloader.download_model(model_name)
            if success:
                break

        results[function_name] = success

        if not success:
            print(f"  ‚ö†Ô∏è  Falha ap√≥s {MAX_DOWNLOAD_RETRIES} tentativas")

    # Relat√≥rio final
    successful = sum(1 for v in results.values() if v)
    print(f"\nüìä Relat√≥rio final:")
    print(f"  ‚úÖ Sucessos: {successful}/{total_models}")
    print(f"  ‚ùå Falhas: {total_models - successful}/{total_models}")

    if successful == total_models:
        print("üéâ Todos os modelos foram baixados com sucesso!")
    else:
        print("‚ö†Ô∏è  Alguns modelos falharam. Verifique logs acima.")

    return results


def list_downloaded_models() -> List[Dict[str, Any]]:
    """
    Lista todos os modelos baixados no cache.

    Returns:
        Lista de dicion√°rios com informa√ß√µes dos modelos
    """
    cache_path = Path(MODEL_CACHE_DIR)
    if not cache_path.exists():
        print("üìÅ Cache de modelos n√£o encontrado.")
        return []

    models = []

    # Busca por diret√≥rios de modelos
    for model_dir in cache_path.rglob("*"):
        if model_dir.is_dir() and (model_dir / "config.json").exists():
            # Calcula tamanho do modelo
            size_bytes = sum(
                f.stat().st_size for f in model_dir.rglob("*") if f.is_file()
            )
            size_mb = size_bytes / (1024 * 1024)

            # Extrai nome do modelo do caminho
            model_name = str(model_dir.relative_to(cache_path))
            if "--" in model_name:
                model_name = model_name.replace("--", "/")

            models.append(
                {
                    "name": model_name,
                    "path": str(model_dir),
                    "size_mb": round(size_mb, 2),
                    "files": len(list(model_dir.rglob("*"))),
                }
            )

    if models:
        print(f"üì¶ Modelos baixados ({len(models)} encontrados):")
        for model in models:
            print(f"  ‚Ä¢ {model['name']} ({model['size_mb']} MB)")
    else:
        print("üì¶ Nenhum modelo encontrado no cache.")

    return models


def validate_model(model_name: str) -> bool:
    """
    Valida se um modelo est√° corretamente baixado e pode ser carregado.

    Args:
        model_name: Nome do modelo para validar

    Returns:
        True se modelo √© v√°lido, False caso contr√°rio
    """
    try:
        # Tenta carregar tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=MODEL_CACHE_DIR,
            local_files_only=True,
        )

        # Tenta carregar configura√ß√£o
        config = AutoConfig.from_pretrained(
            model_name,
            cache_dir=MODEL_CACHE_DIR,
            local_files_only=True,
        )

        # Testa tokeniza√ß√£o simples
        test_text = "Teste de valida√ß√£o"
        tokens = tokenizer.encode(test_text, return_tensors="pt")

        if len(tokens[0]) > 0:
            return True

    except Exception as e:
        print(f"‚ùå Erro na valida√ß√£o do modelo {model_name}: {str(e)}")

    return False


def cleanup_cache() -> Dict[str, Any]:
    """
    Limpa arquivos antigos e corrompidos do cache.

    Returns:
        Estat√≠sticas da limpeza
    """
    cache_path = Path(MODEL_CACHE_DIR)
    if not cache_path.exists():
        return {"cleaned": 0, "freed_mb": 0, "errors": []}

    cleaned_files = 0
    freed_bytes = 0
    errors = []

    print("üßπ Limpando cache de modelos...")

    # Remove arquivos tempor√°rios
    for temp_file in cache_path.rglob("*.tmp"):
        try:
            size = temp_file.stat().st_size
            temp_file.unlink()
            cleaned_files += 1
            freed_bytes += size
            print(f"  üóëÔ∏è  Removido: {temp_file.name}")
        except Exception as e:
            errors.append(f"Erro ao remover {temp_file}: {str(e)}")

    # Remove diret√≥rios vazios
    for empty_dir in cache_path.rglob("*"):
        if empty_dir.is_dir() and not any(empty_dir.iterdir()):
            try:
                empty_dir.rmdir()
                print(f"  üìÅ Diret√≥rio vazio removido: {empty_dir.name}")
            except Exception as e:
                errors.append(f"Erro ao remover diret√≥rio {empty_dir}: {str(e)}")

    freed_mb = freed_bytes / (1024 * 1024)

    print(f"‚ú® Limpeza conclu√≠da:")
    print(f"  ‚Ä¢ Arquivos removidos: {cleaned_files}")
    print(f"  ‚Ä¢ Espa√ßo liberado: {freed_mb:.2f} MB")
    if errors:
        print(f"  ‚Ä¢ Erros: {len(errors)}")

    return {
        "cleaned": cleaned_files,
        "freed_mb": round(freed_mb, 2),
        "errors": errors,
    }


def get_model_info(model_name: str) -> Dict[str, Any]:
    import os
    os.environ["CUDA_VISIBLE_DEVICES"] = ""  # Desativa GPU se tiver
    try:
        config = AutoConfig.from_pretrained(
            model_name,
            cache_dir=MODEL_CACHE_DIR,
            local_files_only=True,
        )

        return {
            "name": model_name,
            "architecture": config.architectures[0] if config.architectures else "unknown",
            "vocab_size": getattr(config, "vocab_size", "unknown"),
            "hidden_size": getattr(config, "hidden_size", "unknown"),
            "num_layers": getattr(config, "num_hidden_layers", "unknown"),
            "max_position": getattr(config, "max_position_embeddings", "unknown"),
            "model_type": getattr(config, "model_type", "unknown"),
        }

    except Exception as e:
        return {
            "name": model_name,
            "error": str(e),
            "available": False,
        }


if __name__ == "__main__":
    import sys

    # Configura√ß√µes para evitar travamentos ao executar o script diretamente
    configure_minimal_resources()
    set_low_priority()

    if len(sys.argv) > 1:
        command = sys.argv[1]

        if command == "download":
            download_default_models()
        elif command == "list":
            list_downloaded_models()
        elif command == "cleanup":
            cleanup_cache()
        elif command == "validate":
            if len(sys.argv) > 2:
                model_name = sys.argv[2]
                is_valid = validate_model(model_name)
                print(
                    f"Modelo {model_name}: {'‚úÖ V√°lido' if is_valid else '‚ùå Inv√°lido'}"
                )
            else:
                print("Usage: python model_downloader.py validate <model_name>")
        else:
            print("Comandos dispon√≠veis: download, list, cleanup, validate")
    else:
        print("Usage: python model_downloader.py <command>")
        print("Comandos: download, list, cleanup, validate")
        print("üí° Dica: Use 'download' para baixar todos os modelos padr√£o com baixo recurso.")
